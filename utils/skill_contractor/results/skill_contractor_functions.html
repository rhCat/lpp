<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8"/>
<title>Function Graph: skill_contractor</title>
<script src="https://d3js.org/d3.v7.min.js"></script>
<style>
body { font-family: 'Segoe UI', Arial, sans-serif; margin: 0; padding: 20px; background: #0f0f23; color: #eee; }
h1 { color: #00d4ff; margin-bottom: 5px; font-size: 24px; }
.subtitle { color: #888; margin-bottom: 15px; font-size: 14px; }
#container { display: flex; gap: 20px; height: calc(100vh - 120px); }
#graph { flex: 1; position: relative; }
#sidebar { width: 350px; background: #1a1a2e; padding: 15px; border-radius: 8px; overflow-y: auto; }
svg { background: #16213e; border-radius: 8px; width: 100%; height: 100%; }

/* Node styles */
.node { cursor: pointer; }
.node-module { fill: #2a2a4a; stroke-width: 3; }
.node-function { fill: #3a3a5a; stroke-width: 2; }
.node-dependency { fill: #1a1a3a; stroke: #666; stroke-width: 1; stroke-dasharray: 4; }
.node-label { font-size: 11px; fill: #fff; pointer-events: none; font-weight: 500; }
.node-sublabel { font-size: 9px; fill: #888; pointer-events: none; }

/* Edge styles */
.edge { fill: none; stroke-opacity: 0.6; }
.edge-internal { stroke: #4ecdc4; stroke-width: 2; }
.edge-external { stroke: #f39c12; stroke-width: 1.5; stroke-dasharray: 4; }
.edge-local { stroke: #9b59b6; stroke-width: 2; }
.edge-label { font-size: 8px; fill: #666; pointer-events: none; }

/* Highlight styles */
.highlight { stroke-width: 3 !important; stroke-opacity: 1 !important; }
.dim { opacity: 0.2; }

/* Controls */
.controls { display: flex; gap: 8px; margin-bottom: 10px; flex-wrap: wrap; }
.controls button { background: #3a3a5a; color: #fff; border: 1px solid #555; padding: 6px 12px; border-radius: 4px; cursor: pointer; font-size: 12px; }
.controls button:hover { background: #4a4a6a; }
.controls button.active { background: #00d4ff; color: #000; border-color: #00d4ff; }

/* Module legend */
.module-legend { margin-top: 15px; }
.module-item { display: flex; align-items: center; gap: 8px; padding: 5px 0; cursor: pointer; }
.module-item:hover { background: #2a2a4a; }
.module-dot { width: 12px; height: 12px; border-radius: 3px; }
.module-name { font-size: 12px; }

/* Info panel */
h3 { color: #00d4ff; margin: 15px 0 8px 0; font-size: 14px; border-bottom: 1px solid #333; padding-bottom: 5px; }
.info-section { font-size: 12px; line-height: 1.6; }
.info-label { color: #888; }
.info-value { color: #fff; }

/* Source code panel */
.source-panel { margin-top: 10px; }
.source-code { background: #0d0d1a; border: 1px solid #333; border-radius: 4px; padding: 10px; font-family: 'Consolas', 'Monaco', monospace; font-size: 11px; line-height: 1.4; overflow-x: auto; max-height: 300px; overflow-y: auto; white-space: pre; color: #b8b8b8; }
.source-header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 5px; }
.source-toggle { background: #3a3a5a; color: #fff; border: none; padding: 4px 8px; border-radius: 3px; cursor: pointer; font-size: 10px; }
.source-toggle:hover { background: #4a4a6a; }
.docstring { color: #6a9955; font-style: italic; }
.metric { display: flex; justify-content: space-between; padding: 3px 0; }
.metric-bar { height: 4px; background: #333; border-radius: 2px; margin-top: 2px; }
.metric-fill { height: 100%; border-radius: 2px; }

/* Edge list */
.edge-list { max-height: 200px; overflow-y: auto; }
.edge-item { padding: 4px 0; border-bottom: 1px solid #333; font-size: 11px; }
.edge-item .from { color: #4ecdc4; }
.edge-item .to { color: #f39c12; }
.edge-item .type { color: #666; font-size: 10px; }

/* Tooltip */
#tooltip { position: absolute; background: #1a1a2e; border: 1px solid #00d4ff; padding: 10px; border-radius: 4px; pointer-events: none; display: none; max-width: 300px; z-index: 100; font-size: 11px; }
</style>
</head>
<body>
<h1>Function Graph: skill_contractor</h1>
<div class="subtitle">Stackable function dependency visualization • Drag nodes to rearrange • Click to inspect</div>

<div class="controls">
  <button onclick="resetView()">Reset View</button>
  <button onclick="fitToView()">Fit</button>
  <button onclick="toggleLayout('force')" id="btn-force" class="active">Force</button>
  <button onclick="toggleLayout('horizontal')" id="btn-horizontal">Horizontal</button>
  <button onclick="toggleLayout('vertical')" id="btn-vertical">Vertical</button>
  <span style="margin-left: 10px; color: #666">|</span>
  <button onclick="toggleEdgeType('internal')" id="btn-internal" class="active">Internal</button>
  <button onclick="toggleEdgeType('external')" id="btn-external" class="active">External</button>
  <button onclick="toggleEdgeType('local')" id="btn-local" class="active">Local</button>
</div>

<div id="container">
  <div id="graph"><svg></svg></div>
  <div id="sidebar">
    <h3>Modules</h3>
    <div class="module-legend" id="module-legend"></div>

    <h3>Selected Node</h3>
    <div class="info-section" id="node-info">Click a node to see details</div>

    <h3>Source Code</h3>
    <div class="source-panel" id="source-panel">
      <div id="source-content" style="color:#666;font-size:11px">Click a function to view source</div>
    </div>

    <h3>Connections</h3>
    <div class="edge-list" id="edge-list"></div>
  </div>
</div>
<div id="tooltip"></div>

<script>
const nodes = [{"id": "skill_contractor_compute", "type": "module", "label": "skill_contractor_compute", "metrics": {"fanIn": 25, "fanOut": 10, "instability": 0.286, "internalEdges": 1, "externalCallCount": 60, "localCallCount": 4, "callsByCategory": {"stdlib": 57, "pip": 3}, "localDependencies": ["", "prompts"]}, "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.init", "type": "function", "label": "init", "direction": "inbound", "parent": "skill_contractor_compute", "line": 544, "endLine": 583, "signature": "(params) -> Dict[]", "docstring": "Initialize agent context with new runtime folder.", "source": "def init(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Initialize agent context with new runtime folder.\"\"\"\n    workspace = os.environ.get(\"WORKSPACE_PATH\", os.getcwd())\n\n    # Create new run folder\n    runDir = _getRunDir(workspace)\n    runId = runDir.name\n\n    _logRun(runDir, f\"=== NEW RUN: {runId} ===\")\n    _logRun(runDir, f\"Workspace: {workspace}\")\n\n    print(f\"  [INIT] Run folder: {runDir}\")\n\n    return {\n        \"api_key\": os.environ.get(\"OPENAI_API_KEY\"),\n        \"api_base\": os.environ.get(\"OPENAI_API_BASE\", \"https://api.openai.com/v1\"),\n        \"model\": os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n        \"workspace_path\": workspace,\n        \"run_id\": runId,\n        \"run_dir\": str(runDir),\n        \"lpp_root\": _findLppRoot(),\n        \"phase\": \"blueprint\",  # Start in blueprint phase\n        \"blueprint_validated\": False,\n        \"threshold\": int(os.environ.get(\"EVAL_THRESHOLD\", \"80\")),\n        \"max_iterations\": int(os.environ.get(\"MAX_ITERATIONS\", \"5\")),\n        \"max_errors\": int(os.environ.get(\"MAX_ERRORS\", \"3\")),\n        \"max_repairs\": int(os.environ.get(\"MAX_REPAIRS\", \"3\")),\n        \"iteration\": 0,\n        \"error_count\": 0,\n        \"step_error_count\": 0,\n        \"repair_attempts\": 0,\n        \"failed_steps\": [],\n        \"execution_log\": [],\n        \"artifacts\": [],\n        \"is_lpp_target\": True,\n        \"lpp_validated\": False,\n        \"raw_output\": None,\n        \"parsed_output\": None,\n        \"parse_error\": None\n    }", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.detect_lpp_target", "type": "function", "label": "detect_lpp_target", "direction": "inbound", "parent": "skill_contractor_compute", "line": 586, "endLine": 601, "signature": "(params) -> Dict[]", "docstring": "Detect if target requires L++ output.", "source": "def detect_lpp_target(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Detect if target requires L++ output.\"\"\"\n    target = (params.get(\"target\") or \"\").lower()\n    analysisOnly = [\"explain\", \"what is\", \"describe\", \"list files\", \"show me\"]\n    codeWords = [\"create\", \"build\", \"make\",\n                 \"generate\", \"write\", \"app\", \"skill\"]\n    isAnalysis = any(k in target for k in analysisOnly) and \\\n        not any(w in target for w in codeWords)\n    isLpp = not isAnalysis\n\n    runDir = Path(params.get(\"run_dir\", \".\"))\n    _logRun(runDir, f\"Target: {target[:100]}...\")\n    _logRun(runDir, f\"Mode: {'L++' if isLpp else 'Analysis'}\")\n\n    print(f\"  [{'L++ MODE' if isLpp else 'ANALYSIS MODE'}]\")\n    return {\"is_lpp_target\": isLpp}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.decompose", "type": "function", "label": "decompose", "direction": "inbound", "parent": "skill_contractor_compute", "line": 604, "endLine": 689, "signature": "(params) -> Dict[]", "docstring": "Decompose target into steps via LLM.", "source": "def decompose(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Decompose target into steps via LLM.\"\"\"\n    apiKey = params.get(\"api_key\")\n    apiBase = params.get(\"api_base\")\n    model = params.get(\"model\")\n    target = params.get(\"target\")\n    workspace = params.get(\"workspace_path\", \".\")\n    feedback = params.get(\"feedback\") or \"\"\n    lppRoot = params.get(\"lpp_root\", \"\")\n    iteration = params.get(\"iteration\", 0)\n    phase = params.get(\"phase\", \"blueprint\")\n    runDir = Path(params.get(\"run_dir\", workspace))\n\n    _logRun(runDir, f\"DECOMPOSE iteration={iteration} phase={phase}\")\n\n    failureCtx = \"\"\n    if feedback:\n        fixHints = []\n        if \"'list' object has no attribute 'keys'\" in feedback:\n            fixHints.append(\n                \"FIX: Use DICT format for states/gates/actions, NOT arrays\")\n        if \"'str' object has no attribute 'get'\" in feedback:\n            fixHints.append(\n                \"FIX: Actions with type 'compute' MUST have input_map and output_map\")\n        if \"input_map\" in feedback or \"output_map\" in feedback:\n            fixHints.append(\n                \"FIX: Every compute action needs input_map:{} and output_map:{}\")\n\n        hintsStr = \"\\n\".join(\n            fixHints) if fixHints else \"Review the error and fix the schema format\"\n\n        failureCtx = f\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551  CRITICAL: ITERATION {iteration} FAILED - YOU MUST FIX THIS  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nERROR DETAILS:\n{_truncate(feedback, 1200)}\n\nREQUIRED FIXES:\n{hintsStr}\n\nDO NOT generate the same broken output!\n\"\"\"\n        _logRun(runDir, f\"Feedback for decompose:\\n{feedback[:500]}\")\n\n    # Select phase-specific instructions\n    phaseInstructions = prompts.PHASE_BLUEPRINT_INSTRUCTIONS if phase == \"blueprint\" else prompts.PHASE_IMPLEMENTATION_INSTRUCTIONS\n\n    prompt = prompts.DECOMPOSE.format(\n        target=target,\n        workspace=workspace,\n        iteration=iteration + 1,\n        phase=phase,\n        phase_instructions=phaseInstructions,\n        failure_ctx=failureCtx,\n        lpp_root=lppRoot,\n        # Only include LPP_RULES in blueprint phase\n        lpp_rules=prompts.LPP_RULES if phase == \"blueprint\" else \"\",\n        json_rules=prompts.JSON_RULES\n    )\n\n    result = _callLlm(apiKey, apiBase, model,\n                      [{\"role\": \"system\", \"content\": prompts.SYSTEM},\n                       {\"role\": \"user\", \"content\": prompt}], 0.3, 2048)\n\n    if result.get(\"error\"):\n        _logRun(runDir, f\"DECOMPOSE ERROR: {result['error']}\")\n        return {\"plan\": None, \"step_count\": 0, \"step_index\": 0,\n                \"current_step\": None, \"error\": result[\"error\"]}\n\n    try:\n        plan = _extractJson(result[\"response\"])\n        steps = plan.get(\"steps\", [])\n        _logRun(runDir, f\"DECOMPOSE OK: {len(steps)} steps\")\n        return {\n            \"plan\": plan,\n            \"step_count\": len(steps),\n            \"step_index\": 0,\n            \"current_step\": steps[0] if steps else None,\n            \"error\": None\n        }\n    except ValueError as e:\n        _logRun(runDir, f\"DECOMPOSE PARSE ERROR: {e}\")\n        return {\"plan\": None, \"step_count\": 0, \"step_index\": 0,\n                \"current_step\": None, \"error\": f\"Parse error: {e}\"}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.generate_step_output", "type": "function", "label": "generate_step_output", "direction": "inbound", "parent": "skill_contractor_compute", "line": 692, "endLine": 839, "signature": "(params) -> Dict[]", "docstring": "Generate step output via LLM with step-level logging.", "source": "def generate_step_output(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Generate step output via LLM with step-level logging.\"\"\"\n    apiKey = params.get(\"api_key\")\n    apiBase = params.get(\"api_base\")\n    model = params.get(\"model\")\n    target = params.get(\"target\")\n    step = params.get(\"current_step\")\n    stepIdx = params.get(\"step_index\", 0)\n    workspace = params.get(\"workspace_path\", \".\")\n    execLog = params.get(\"execution_log\", [])\n    lppRoot = params.get(\"lpp_root\", \"\")\n    parseError = params.get(\"parse_error\")\n    rawOutput = params.get(\"raw_output\")\n    repairAttempts = params.get(\"repair_attempts\", 0)\n    phase = params.get(\"phase\", \"blueprint\")\n    runDir = Path(params.get(\"run_dir\", workspace))\n\n    if not step:\n        return {\"raw_output\": None, \"error\": \"No step\"}\n\n    # Output goes to runs/<run_id>/output/ for cleaner organization\n    outputDir = runDir / \"output\"\n    outputDir.mkdir(exist_ok=True)\n    plan = params.get(\"plan\", {})\n    iterFeedback = params.get(\"feedback\") or \"\"\n\n    # Log step start\n    _logStep(runDir, stepIdx, \"GENERATE_START\",\n             f\"Action: {step.get('action', 'N/A')}\\nType: {step.get('type', 'N/A')}\\nPhase: {phase}\\nRepair attempt: {repairAttempts}\")\n\n    # Read previous log for this step to give LLM context\n    prevLog = _readStepLog(runDir, stepIdx, maxChars=4000)\n\n    ctx = _condenseForStep(plan, stepIdx, execLog, iterFeedback)\n\n    # Build context for LLM\n    iterCtx = \"\"\n\n    if parseError and rawOutput:\n        # Include step log for better context\n        logContext = f\"\\n\\nPREVIOUS ATTEMPTS LOG:\\n{prevLog}\" if prevLog else \"\"\n\n        iterCtx = f\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551  \u26a0\ufe0f  OUTPUT PARSING FAILED (attempt {repairAttempts}) - FIX IT  \u26a0\ufe0f  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nPARSE ERROR:\n{_truncate(parseError, 800)}\n\nYOUR BROKEN OUTPUT:\n```\n{_truncate(rawOutput, 4000)}\n```\n{logContext}\n\nFIX THE JSON STRUCTURE! For L++ blueprints you MUST include:\n- \"$schema\": \"lpp/v0.1.2\"\n- \"id\", \"name\", \"version\", \"description\"\n- \"states\": {{\"state_name\": {{\"description\": \"...\"}}, ...}}  (DICT not array!)\n- \"gates\": {{\"gate_name\": {{\"type\": \"expression\", \"expression\": \"...\"}}, ...}}\n- \"actions\": {{\"action_name\": {{\"type\": \"compute|set\", ...}}, ...}}\n- \"transitions\": [...]\n- \"entry_state\", \"terminal_states\"\n\"\"\"\n        _logStep(runDir, stepIdx, \"PARSE_ERROR_CONTEXT\",\n                 f\"Error: {parseError}\\nBroken output length: {len(rawOutput or '')}\")\n\n    elif ctx.get(\"iteration_feedback\"):\n        fb = ctx['iteration_feedback']\n        lastOut = ctx.get(\"last_output\") or \"\"\n\n        fixHints = []\n        if \"'list' object has no attribute 'keys'\" in fb:\n            fixHints.append(\n                \"\u2192 WRONG: \\\"states\\\": [{...}]  \u2192  RIGHT: \\\"states\\\": {\\\"idle\\\": {...}}\")\n        if \"'str' object has no attribute 'get'\" in fb:\n            fixHints.append(\n                \"\u2192 Actions MUST have: \\\"input_map\\\": {...}, \\\"output_map\\\": {...}\")\n\n        hintsStr = \"\\n\".join(\n            fixHints) if fixHints else \"Fix the validation error below\"\n        brokenOutput = f\"\\nYOUR PREVIOUS OUTPUT:\\n```\\n{_truncate(lastOut, 4000)}\\n```\" if lastOut else \"\"\n\n        iterCtx = f\"\"\"\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n\u2551  \u26a0\ufe0f  PREVIOUS ATTEMPT FAILED - MUST FIX BEFORE PROCEEDING  \u26a0\ufe0f  \u2551\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n\nVALIDATION ERROR:\n{_truncate(fb, 1000)}\n\nSPECIFIC FIXES REQUIRED:\n{hintsStr}\n{brokenOutput}\n\"\"\"\n\n    lastAttemptStr = \"First attempt\"\n    if ctx[\"last_attempt\"]:\n        la = ctx[\"last_attempt\"]\n        lastAttemptStr = f\"Attempt: {la.get('result', 'N/A')[:80]}, success: {la.get('success', 'N/A')}\"\n\n    # Select phase-specific instructions and output format\n    if phase == \"blueprint\":\n        phaseInstructions = prompts.PHASE_BLUEPRINT_INSTRUCTIONS\n        phaseOutputFormat = prompts.BLUEPRINT_OUTPUT_FORMAT\n        lppRules = prompts.LPP_RULES\n    else:\n        phaseInstructions = prompts.PHASE_IMPLEMENTATION_INSTRUCTIONS\n        phaseOutputFormat = prompts.IMPLEMENTATION_OUTPUT_FORMAT\n        lppRules = \"\"  # Don't include L++ schema rules in implementation phase\n\n    prompt = prompts.EXECUTE_STEP.format(\n        target=target,\n        phase=phase.upper(),\n        phase_instructions=phaseInstructions,\n        phase_output_format=phaseOutputFormat,\n        progress=ctx[\"progress\"],\n        action=step.get(\"action\", \"\"),\n        step_type=step.get(\"type\", \"analyze\"),\n        output_dir=str(outputDir),\n        lpp_root=lppRoot,\n        iteration_ctx=iterCtx,\n        prev_step=ctx[\"prev\"][\"action\"] if ctx[\"prev\"] else \"None\",\n        next_step=ctx[\"next\"][\"action\"] if ctx[\"next\"] else \"Final step\",\n        last_attempt=lastAttemptStr,\n        lpp_rules=lppRules,\n        json_rules=prompts.JSON_RULES\n    )\n\n    _logStep(runDir, stepIdx, \"LLM_PROMPT\",\n             f\"Prompt length: {len(prompt)} chars\")\n\n    result = _callLlm(apiKey, apiBase, model,\n                      [{\"role\": \"system\", \"content\": prompts.SYSTEM},\n                       {\"role\": \"user\", \"content\": prompt}], 0.4, 4096)\n\n    if result.get(\"error\"):\n        _logStep(runDir, stepIdx, \"LLM_ERROR\", result[\"error\"])\n        return {\"raw_output\": None, \"error\": result[\"error\"]}\n\n    rawResp = result.get(\"response\", \"\")\n    _logStep(runDir, stepIdx, \"LLM_RESPONSE\",\n             f\"Response length: {len(rawResp)} chars\\n\\n{rawResp[:2000]}\")\n\n    print(f\"  [GENERATE] Step {stepIdx + 1}: {len(rawResp)} chars\")\n\n    return {\"raw_output\": rawResp, \"error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.parse_and_sanitize", "type": "function", "label": "parse_and_sanitize", "direction": "inbound", "parent": "skill_contractor_compute", "line": 842, "endLine": 933, "signature": "(params) -> Dict[]", "docstring": "Parse and sanitize LLM output with logging.", "source": "def parse_and_sanitize(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Parse and sanitize LLM output with logging.\"\"\"\n    rawOutput = params.get(\"raw_output\", \"\")\n    step = params.get(\"current_step\", {})\n    isLppTarget = params.get(\"is_lpp_target\", True)\n    phase = params.get(\"phase\", \"blueprint\")\n    stepType = step.get(\"type\", \"analyze\") if step else \"analyze\"\n    isLppStep = \"lpp\" in stepType.lower() if stepType else False\n    stepIdx = params.get(\"step_index\", 0)\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    # In implementation phase, we're generating Python code, not blueprints\n    inBlueprintPhase = (phase == \"blueprint\")\n\n    if not rawOutput:\n        _logStep(runDir, stepIdx, \"PARSE_FAIL\", \"Empty LLM response\")\n        return {\"parsed_output\": None, \"parse_error\": \"Empty LLM response\"}\n\n    # Step 1: Extract JSON\n    try:\n        parsed = _extractJson(rawOutput)\n        _logStep(runDir, stepIdx, \"JSON_EXTRACTED\",\n                 f\"Keys: {list(parsed.keys())[:10]} Phase: {phase}\")\n    except ValueError as e:\n        _logStep(runDir, stepIdx, \"JSON_EXTRACT_FAIL\",\n                 f\"Error: {e}\\nRaw start: {rawOutput[:500]}\")\n        return {\"parsed_output\": None,\n                \"parse_error\": f\"JSON extraction failed: {e}. Raw output starts with: {rawOutput[:200]}\"}\n\n    # Step 2: In blueprint phase, check if blueprint is nested in 'output' field\n    # This happens regardless of stepType since LLM may return {\"output\": \"<escaped blueprint>\"}\n    if inBlueprintPhase and \"output\" in parsed and isinstance(parsed[\"output\"], str):\n        outputStr = parsed[\"output\"]\n        # Unescape the nested JSON string\n        outputStr = outputStr.replace(\"\\\\\\\\n\", \"\\n\").replace(\"\\\\\\\\t\", \"\\t\")\n        outputStr = outputStr.replace('\\\\\\\\\"', '\"').replace(\"\\\\'\", \"'\")\n        outputStr = outputStr.replace(\"\\\\n\", \"\\n\").replace(\"\\\\t\", \"\\t\")\n        outputStr = outputStr.replace('\\\\\"', '\"')\n\n        # Try to parse the nested blueprint\n        if outputStr.strip().startswith(\"{\") and (\"$schema\" in outputStr or \"states\" in outputStr):\n            try:\n                innerParsed = json.loads(outputStr)\n                _logStep(runDir, stepIdx, \"NESTED_BLUEPRINT\",\n                         f\"Extracted from output field, keys: {list(innerParsed.keys())[:8]}\")\n                # Use the inner blueprint but preserve filename\n                innerParsed[\"_filename\"] = parsed.get(\n                    \"filename\", \"blueprint.json\")\n                parsed = innerParsed\n            except json.JSONDecodeError as e:\n                _logStep(runDir, stepIdx, \"NESTED_PARSE_FAIL\",\n                         f\"Inner JSON parse failed: {e}\\nStart: {outputStr[:300]}\")\n                # Continue with outer parsed, will fail blueprint validation\n\n    # Step 3: Detect blueprint - ONLY in blueprint phase\n    isBlueprint = inBlueprintPhase and (\n        isLppStep or\n        \"$schema\" in parsed or\n        (\"states\" in parsed and \"transitions\" in parsed)\n    )\n\n    # Step 4: Sanitize based on what we have\n    if isBlueprint and isLppTarget:\n        sanitized, corrections, error = _sanitizeBlueprint(parsed)\n\n        # Log corrections for review\n        if corrections:\n            report = _formatCorrectionsReport(corrections)\n            _logStep(runDir, stepIdx, \"BLUEPRINT_CORRECTIONS\", report)\n            print(f\"\\n{report}\\n\")\n\n        if error:\n            _logStep(runDir, stepIdx, \"BLUEPRINT_INVALID\", error)\n            return {\"parsed_output\": None,\n                    \"parse_error\": f\"Blueprint schema errors (cannot auto-fix):\\n{error}\",\n                    \"corrections\": corrections}\n        # Restore filename if we extracted from nested\n        if \"_filename\" in parsed:\n            sanitized[\"_filename\"] = parsed[\"_filename\"]\n        _logStep(runDir, stepIdx, \"BLUEPRINT_VALID\",\n                 f\"States: {list(sanitized.get('states', {}).keys())}, Corrections: {len(corrections)}\")\n        return {\"parsed_output\": sanitized, \"parse_error\": None, \"corrections\": corrections}\n    else:\n        # Implementation phase or non-blueprint step - treat as code/file output\n        sanitized, error = _sanitizeStepOutput(parsed, stepType)\n        if error:\n            _logStep(runDir, stepIdx, \"OUTPUT_INVALID\", error)\n            return {\"parsed_output\": None,\n                    \"parse_error\": f\"Step output errors:\\n{error}\"}\n        _logStep(runDir, stepIdx, \"OUTPUT_VALID\",\n                 f\"Filename: {sanitized.get('filename', 'N/A')} Phase: {phase}\")\n        return {\"parsed_output\": sanitized, \"parse_error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.write_output", "type": "function", "label": "write_output", "direction": "inbound", "parent": "skill_contractor_compute", "line": 936, "endLine": 1037, "signature": "(params) -> Dict[]", "docstring": "Write validated output to disk.", "source": "def write_output(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Write validated output to disk.\"\"\"\n    parsed = params.get(\"parsed_output\", {})\n    step = params.get(\"current_step\", {})\n    stepIdx = params.get(\"step_index\", 0)\n    workspace = params.get(\"workspace_path\", \".\")\n    execLog = params.get(\"execution_log\", [])\n    artifacts = params.get(\"artifacts\", [])\n    runDir = Path(params.get(\"run_dir\", workspace))\n\n    if not parsed:\n        return {\"execution_log\": execLog, \"artifacts\": artifacts,\n                \"current_step\": step, \"error\": \"No parsed output\"}\n\n    # Output goes to runs/<run_id>/output/ for cleaner organization\n    outputDir = runDir / \"output\"\n    outputDir.mkdir(exist_ok=True)\n\n    stepType = step.get(\"type\", \"analyze\") if step else \"analyze\"\n    # Normalize step type - check if it contains 'lpp'\n    isLppStep = \"lpp\" in stepType.lower() if stepType else False\n    filesWritten = []\n\n    # Handle blueprint output - check for $schema regardless of stepType\n    # (LLM may use \"file\" type but still generate a valid blueprint)\n    if \"$schema\" in parsed:\n        filename = parsed.pop(\"_filename\", None) or parsed.get(\n            \"filename\", f\"{parsed.get('id', 'blueprint')}.json\")\n        # Ensure .json extension\n        if not filename.endswith(\".json\"):\n            filename = f\"{filename}.json\"\n        # Write the blueprint JSON directly\n        bpCopy = {k: v for k, v in parsed.items() if not k.startswith(\"_\")}\n        fp = outputDir / filename\n        fp.parent.mkdir(parents=True, exist_ok=True)\n        fp.write_text(json.dumps(bpCopy, indent=2))\n        filesWritten.append(str(fp))\n        _logStep(runDir, stepIdx, \"BLUEPRINT_WRITTEN\", f\"Path: {fp}\")\n        print(f\"  [WROTE BLUEPRINT] {fp}\")\n    else:\n        # Handle regular code/file output\n        output = parsed.get(\"output\", \"\")\n        filename = parsed.get(\"filename\")\n\n        # Unescape\n        if output:\n            output = output.replace(\"\\\\\\\\\", \"\\x00BACKSLASH\\x00\")\n            output = output.replace('\\\\\"', '\"')\n            output = output.replace(\"\\\\n\", \"\\n\")\n            output = output.replace(\"\\\\t\", \"\\t\")\n            output = output.replace(\"\\x00BACKSLASH\\x00\", \"\\\\\")\n\n        # Also check if this looks like a JSON blueprint in the output field\n        isJsonFile = filename and filename.endswith(\".json\")\n        isPythonFile = filename and filename.endswith(\".py\")\n        if output and filename and (isLppStep or stepType in (\"code\", \"file\") or isJsonFile):\n            # Sanitize Python files to fix common LLM errors (literal newlines in strings)\n            if isPythonFile and HAS_SANITIZER:\n                sanitized_output, fixes = sanitize_python_code(output, filename, verbose=True)\n                if fixes:\n                    _logStep(runDir, stepIdx, \"SANITIZED\", f\"{filename}: {', '.join(fixes)}\")\n                    print(f\"  [SANITIZED] {filename}: {', '.join(fixes)}\")\n                output = sanitized_output\n            fp = outputDir / filename\n            fp.parent.mkdir(parents=True, exist_ok=True)\n            fp.write_text(output)\n            filesWritten.append(str(fp))\n            _logStep(runDir, stepIdx, \"FILE_WRITTEN\",\n                     f\"Path: {fp}\\nSize: {len(output)} bytes\")\n            print(f\"  [WROTE] {fp}\")\n\n    if stepType == \"command\" and parsed.get(\"command\"):\n        cmd = parsed[\"command\"]\n        try:\n            proc = subprocess.run(cmd, shell=True, cwd=str(outputDir),\n                                  capture_output=True, text=True, timeout=30)\n            parsed[\"cmd_output\"] = proc.stdout or proc.stderr\n            _logStep(runDir, stepIdx, \"COMMAND_RUN\",\n                     f\"Cmd: {cmd}\\nExit: {proc.returncode}\\nOutput: {parsed['cmd_output'][:500]}\")\n        except Exception as e:\n            parsed[\"cmd_output\"] = str(e)\n            _logStep(runDir, stepIdx, \"COMMAND_ERROR\", str(e))\n\n    logEntry = {\n        \"step_index\": stepIdx,\n        \"step\": step,\n        \"result\": parsed.get(\"result\", \"\"),\n        \"raw_output\": params.get(\"raw_output\", \"\"),\n        \"files_written\": filesWritten,\n        \"success\": True\n    }\n    execLog = list(execLog) + [logEntry]\n    artifacts = list(artifacts) + filesWritten\n\n    step[\"status\"] = \"done\"\n    step[\"output\"] = parsed.get(\"output\", \"\") if not (\n        \"$schema\" in parsed) else json.dumps(parsed, indent=2)[:500]\n\n    _logRun(runDir, f\"Step {stepIdx + 1} COMPLETE: {filesWritten}\")\n\n    return {\"execution_log\": execLog, \"artifacts\": artifacts,\n            \"current_step\": step, \"error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.advance_step", "type": "function", "label": "advance_step", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1040, "endLine": 1051, "signature": "(params) -> Dict[]", "docstring": "Advance to next step.", "source": "def advance_step(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Advance to next step.\"\"\"\n    plan = params.get(\"plan\", {})\n    stepIdx = params.get(\"step_index\", 0)\n    steps = plan.get(\"steps\", []) if plan else []\n    nextIdx = stepIdx + 1\n    nextStep = steps[nextIdx] if nextIdx < len(steps) else None\n\n    runDir = Path(params.get(\"run_dir\", \".\"))\n    _logRun(runDir, f\"ADVANCE: {stepIdx} -> {nextIdx}\")\n\n    return {\"step_index\": nextIdx, \"current_step\": nextStep}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.validate_lpp", "type": "function", "label": "validate_lpp", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1054, "endLine": 1134, "signature": "(params) -> Dict[]", "docstring": "Validate L++ artifacts.", "source": "def validate_lpp(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Validate L++ artifacts.\"\"\"\n    workspace = params.get(\"workspace_path\", \".\")\n    artifacts = params.get(\"artifacts\", [])\n    lppRoot = params.get(\"lpp_root\", \"\")\n    phase = params.get(\"phase\", \"blueprint\")\n    runDir = Path(params.get(\"run_dir\", workspace))\n\n    blueprints = [a for a in artifacts if a.endswith(\".json\")]\n    if not blueprints:\n        _logRun(runDir, \"VALIDATE: No blueprints found\")\n        return {\"lpp_validated\": False, \"blueprint_validated\": False,\n                \"feedback\": \"No blueprints found\", \"error\": None}\n\n    # Output is in runs/<run_id>/output/\n    outputDir = runDir / \"output\"\n    skillDir = str(outputDir)\n    for bp in blueprints:\n        bpPath = Path(bp)\n        if bpPath.exists():\n            parent = bpPath.parent\n            if (parent / \"src\").exists():\n                skillDir = str(parent)\n                break\n\n    buildScript = None\n    for p in [Path(lppRoot) / \"utils\" / \"build_skill.sh\" if lppRoot else None,\n              Path(workspace).parent / \"utils\" / \"build_skill.sh\"]:\n        if p and p.exists():\n            buildScript = p\n            break\n\n    if not buildScript:\n        _logRun(runDir, \"VALIDATE: build_skill.sh not found, skipping\")\n        return {\"lpp_validated\": True, \"blueprint_validated\": True,\n                \"feedback\": None, \"error\": None}\n\n    runCwd = lppRoot or str(Path(workspace).parent)\n    _logRun(runDir, f\"VALIDATE [{phase}]: {buildScript} {skillDir} --validate\")\n\n    try:\n        proc = subprocess.run(\n            [str(buildScript), skillDir, \"--validate\"],\n            cwd=runCwd, capture_output=True, text=True, timeout=60\n        )\n        output = proc.stdout + proc.stderr\n\n        if \"PASS\" in output:\n            _logRun(runDir, f\"VALIDATE [{phase}]: PASSED\")\n            # Both phases set lpp_validated=True on pass, but blueprint_validated tracks blueprint specifically\n            if phase == \"blueprint\":\n                return {\"lpp_validated\": True, \"blueprint_validated\": True,\n                        \"feedback\": f\"Blueprint phase PASSED - advancing to implementation\", \"error\": None}\n            else:\n                return {\"lpp_validated\": True, \"blueprint_validated\": True,\n                        \"feedback\": None, \"error\": None}\n        else:\n            _logRun(runDir, f\"VALIDATE [{phase}]: FAILED\\n{output[:1000]}\")\n            errLines = [l.strip() for l in output.split(\"\\n\")\n                        if any(k in l.lower() for k in [\"error\", \"violation\", \"fail\", \"missing\"])]\n            errDetail = \"\\n\".join(\n                errLines[-10:]) if errLines else output[-600:]\n\n            schemaHints = []\n            if \"'list' object has no attribute 'keys'\" in output:\n                schemaHints.append(\n                    \"Use DICT not ARRAY for states/gates/actions\")\n            if \"'str' object has no attribute 'get'\" in output:\n                schemaHints.append(\n                    \"transitions MUST be an ARRAY, not a dict! Use: \\\"transitions\\\": [{\\\"id\\\": \\\"t1\\\", \\\"from\\\": ..., \\\"to\\\": ..., \\\"on_event\\\": ...}, ...]\")\n            if \"AttributeError\" in output and \"transitions\" in output:\n                schemaHints.append(\n                    \"transitions MUST be: [{\\\"id\\\": \\\"t1\\\", \\\"from\\\": \\\"state1\\\", \\\"to\\\": \\\"state2\\\", \\\"on_event\\\": \\\"event_name\\\"}, ...]\")\n\n            feedback = f\"L++ VALIDATION FAILED [{phase}]:\\n{errDetail}\\n{chr(10).join(schemaHints)}\"\n            return {\"lpp_validated\": False, \"blueprint_validated\": False,\n                    \"feedback\": feedback, \"error\": None}\n    except Exception as e:\n        _logRun(runDir, f\"VALIDATE ERROR: {e}\")\n        return {\"lpp_validated\": False, \"blueprint_validated\": False,\n                \"feedback\": f\"Validation exception: {e}\", \"error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.advance_phase", "type": "function", "label": "advance_phase", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1137, "endLine": 1150, "signature": "(params) -> Dict[]", "docstring": "Advance from blueprint phase to implementation phase.", "source": "def advance_phase(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Advance from blueprint phase to implementation phase.\"\"\"\n    currentPhase = params.get(\"phase\", \"blueprint\")\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    if currentPhase == \"blueprint\":\n        newPhase = \"implementation\"\n        _logRun(runDir, f\"PHASE ADVANCE: {currentPhase} -> {newPhase}\")\n        print(f\"  [PHASE] Blueprint validated - advancing to implementation\")\n        return {\"phase\": newPhase}\n    else:\n        # Already in implementation phase\n        _logRun(runDir, f\"PHASE: Already in {currentPhase}\")\n        return {\"phase\": currentPhase}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.evaluate", "type": "function", "label": "evaluate", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1153, "endLine": 1213, "signature": "(params) -> Dict[]", "docstring": "Self-evaluate progress.", "source": "def evaluate(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Self-evaluate progress.\"\"\"\n    apiKey = params.get(\"api_key\")\n    apiBase = params.get(\"api_base\")\n    model = params.get(\"model\")\n    target = params.get(\"target\")\n    plan = params.get(\"plan\")\n    execLog = params.get(\"execution_log\", [])\n    artifacts = params.get(\"artifacts\", [])\n    iteration = params.get(\"iteration\", 0)\n    threshold = params.get(\"threshold\", 80)\n    isLppTarget = params.get(\"is_lpp_target\", True)\n    lppValidated = params.get(\"lpp_validated\", False)\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    lppStatus = \"PASSED\" if lppValidated else (\n        \"REQUIRED\" if isLppTarget else \"N/A\")\n    ctx = _condenseForEval(plan, execLog, artifacts)\n\n    _logRun(\n        runDir, f\"EVALUATE: iter={iteration}, lpp={lppStatus}, completed={ctx['completed']}/{ctx['total_steps']}\")\n\n    prompt = prompts.EVALUATE.format(\n        target=target,\n        iteration=iteration + 1,\n        threshold=threshold,\n        lpp_status=lppStatus,\n        total_steps=ctx[\"total_steps\"],\n        completed=ctx[\"completed\"],\n        pending=ctx[\"pending\"],\n        results=json.dumps(ctx[\"results\"], indent=2),\n        artifacts=\", \".join(ctx[\"artifacts\"]) if ctx[\"artifacts\"] else \"None\",\n        json_rules=prompts.JSON_RULES\n    )\n\n    result = _callLlm(apiKey, apiBase, model,\n                      [{\"role\": \"system\", \"content\": prompts.SYSTEM},\n                       {\"role\": \"user\", \"content\": prompt}], 0.2, 2048)\n\n    if result.get(\"error\"):\n        _logRun(runDir, f\"EVALUATE ERROR: {result['error']}\")\n        return {\"evaluation\": None, \"score\": 0, \"is_satisfied\": False,\n                \"feedback\": None, \"error\": result[\"error\"]}\n\n    try:\n        ev = _extractJson(result[\"response\"])\n        score = ev.get(\"score\", 0)\n        satisfied = ev.get(\"satisfied\", False) or score >= threshold\n        _logRun(\n            runDir, f\"EVALUATE RESULT: score={score}, satisfied={satisfied}\")\n        return {\n            \"evaluation\": ev,\n            \"score\": score,\n            \"is_satisfied\": satisfied,\n            \"feedback\": ev.get(\"feedback\"),\n            \"error\": None\n        }\n    except ValueError as e:\n        _logRun(runDir, f\"EVALUATE PARSE ERROR: {e}\")\n        return {\"evaluation\": None, \"score\": 0, \"is_satisfied\": False,\n                \"feedback\": None, \"error\": f\"Parse error: {e}\"}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.incr_iteration", "type": "function", "label": "incr_iteration", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1220, "endLine": 1221, "signature": "(params) -> Dict[]", "docstring": null, "source": "def incr_iteration(params: Dict[str, Any]) -> Dict[str, Any]:\n    return {\"iteration\": params.get(\"iteration\", 0) + 1}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.incr_repair", "type": "function", "label": "incr_repair", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1224, "endLine": 1225, "signature": "(params) -> Dict[]", "docstring": null, "source": "def incr_repair(params: Dict[str, Any]) -> Dict[str, Any]:\n    return {\"repair_attempts\": params.get(\"repair_attempts\", 0) + 1}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.reset_for_refine", "type": "function", "label": "reset_for_refine", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1228, "endLine": 1229, "signature": "(params) -> Dict[]", "docstring": null, "source": "def reset_for_refine(params: Dict[str, Any]) -> Dict[str, Any]:\n    return {\"step_index\": 0, \"current_step\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.capture_error", "type": "function", "label": "capture_error", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1232, "endLine": 1246, "signature": "(params) -> Dict[]", "docstring": null, "source": "def capture_error(params: Dict[str, Any]) -> Dict[str, Any]:\n    error = params.get(\"error\", \"Unknown\")\n    count = params.get(\"error_count\", 0) + 1\n    step = params.get(\"current_step\", {})\n    stepIdx = params.get(\"step_index\", 0)\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    stepInfo = f\" at step {stepIdx + 1}: {step.get('action', '')}\" if step else \"\"\n    _logRun(runDir, f\"ERROR ({count}): {error}\")\n\n    return {\n        \"last_error\": error,\n        \"feedback\": f\"Error{stepInfo}: {error}. Adjust plan.\",\n        \"error_count\": count\n    }", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.capture_step_error", "type": "function", "label": "capture_step_error", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1249, "endLine": 1261, "signature": "(params) -> Dict[]", "docstring": null, "source": "def capture_step_error(params: Dict[str, Any]) -> Dict[str, Any]:\n    error = params.get(\"error\", \"Unknown\")\n    count = params.get(\"step_error_count\", 0) + 1\n    stepIdx = params.get(\"step_index\", 0)\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    _logStep(runDir, stepIdx, \"STEP_ERROR\", f\"Count: {count}\\nError: {error}\")\n\n    return {\n        \"last_error\": error,\n        \"feedback\": f\"Step error: {error}\",\n        \"step_error_count\": count\n    }", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.capture_parse_error", "type": "function", "label": "capture_parse_error", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1264, "endLine": 1277, "signature": "(params) -> Dict[]", "docstring": null, "source": "def capture_parse_error(params: Dict[str, Any]) -> Dict[str, Any]:\n    parseError = params.get(\"parse_error\", \"Unknown parse error\")\n    rawOutput = params.get(\"raw_output\", \"\")\n    repairAttempts = params.get(\"repair_attempts\", 0)\n    stepIdx = params.get(\"step_index\", 0)\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    _logStep(runDir, stepIdx, \"PARSE_ERROR\",\n             f\"Attempt: {repairAttempts}\\nError: {parseError}\")\n\n    return {\n        \"feedback\": f\"Parse error (attempt {repairAttempts}): {parseError}\",\n        \"step_error_count\": 0\n    }", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.prepare_recovery", "type": "function", "label": "prepare_recovery", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1280, "endLine": 1288, "signature": "(params) -> Dict[]", "docstring": null, "source": "def prepare_recovery(params: Dict[str, Any]) -> Dict[str, Any]:\n    feedback = params.get(\"feedback\", \"\")\n    plan = params.get(\"plan\", {})\n    stepIdx = params.get(\"step_index\", 0)\n    if plan:\n        steps = plan.get(\"steps\", [])[stepIdx:]\n        if steps:\n            feedback += f\" Failed: {[s.get('action') for s in steps[:3]]}\"\n    return {\"feedback\": feedback, \"error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.review_failed_step", "type": "function", "label": "review_failed_step", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1291, "endLine": 1348, "signature": "(params) -> Dict[]", "docstring": "LLM reviews failed step: skip or replan.", "source": "def review_failed_step(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"LLM reviews failed step: skip or replan.\"\"\"\n    apiKey = params.get(\"api_key\")\n    apiBase = params.get(\"api_base\")\n    model = params.get(\"model\")\n    target = params.get(\"target\")\n    step = params.get(\"current_step\") or {}\n    stepIdx = params.get(\"step_index\", 0)\n    stepCount = params.get(\"step_count\", 0)\n    lastError = params.get(\"last_error\", \"Unknown\")\n    failedSteps = params.get(\"failed_steps\") or []\n    execLog = params.get(\"execution_log\") or []\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    ctx = _condenseForReview(step, stepIdx, execLog, lastError)\n\n    # Include step log for context\n    stepLog = _readStepLog(runDir, stepIdx, maxChars=2000)\n\n    _logRun(runDir, f\"REVIEW: step {stepIdx + 1}, error: {lastError[:100]}\")\n\n    prompt = prompts.REVIEW_STEP.format(\n        target=target,\n        step_num=f\"{ctx['step_num']}/{stepCount}\",\n        action=ctx[\"action\"],\n        error=ctx[\"last_error\"],\n        attempts=ctx[\"attempts\"],\n        attempt_results=json.dumps(ctx[\"attempt_results\"], indent=2),\n        failed_steps=len(failedSteps),\n        json_rules=prompts.JSON_RULES\n    )\n\n    # Add step log context\n    if stepLog:\n        prompt += f\"\\n\\nSTEP LOG:\\n{stepLog}\"\n\n    result = _callLlm(apiKey, apiBase, model,\n                      [{\"role\": \"system\", \"content\": prompts.SYSTEM},\n                       {\"role\": \"user\", \"content\": prompt}], 0.3, 1024)\n\n    if result.get(\"error\"):\n        _logRun(runDir, \"REVIEW: LLM error, defaulting to skip\")\n        return {\"review_decision\": \"skip\", \"feedback\": f\"Skipping: {lastError}\",\n                \"failed_steps\": failedSteps + [step]}\n\n    try:\n        rv = _extractJson(result[\"response\"])\n        decision = rv.get(\"decision\", \"skip\")\n        if decision not in (\"skip\", \"replan\"):\n            decision = \"skip\"\n        _logRun(runDir, f\"REVIEW DECISION: {decision}\")\n        newFailed = failedSteps + [step] if decision == \"skip\" else failedSteps\n        return {\"review_decision\": decision, \"feedback\": rv.get(\"feedback\", \"\"),\n                \"failed_steps\": newFailed}\n    except ValueError:\n        _logRun(runDir, \"REVIEW: Parse error, defaulting to skip\")\n        return {\"review_decision\": \"skip\", \"feedback\": f\"Skipping: {lastError}\",\n                \"failed_steps\": failedSteps + [step]}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.skip_step", "type": "function", "label": "skip_step", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1351, "endLine": 1364, "signature": "(params) -> Dict[]", "docstring": "Skip current step and advance.", "source": "def skip_step(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Skip current step and advance.\"\"\"\n    plan = params.get(\"plan\", {})\n    stepIdx = params.get(\"step_index\", 0)\n    failedSteps = params.get(\"failed_steps\") or []\n    steps = plan.get(\"steps\", []) if plan else []\n    nextIdx = stepIdx + 1\n    nextStep = steps[nextIdx] if nextIdx < len(steps) else None\n    runDir = Path(params.get(\"run_dir\", \".\"))\n\n    _logRun(runDir, f\"SKIP: step {stepIdx + 1} -> {nextIdx + 1}\")\n\n    return {\"step_index\": nextIdx, \"current_step\": nextStep,\n            \"failed_steps\": failedSteps, \"step_error_count\": 0}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.save_state", "type": "function", "label": "save_state", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1371, "endLine": 1397, "signature": "(params) -> Dict[]", "docstring": "Save state to run folder.", "source": "def save_state(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Save state to run folder.\"\"\"\n    runDir = Path(params.get(\"run_dir\", params.get(\"workspace_path\", \".\")))\n    path = runDir / \"state.json\"\n\n    data = {\n        \"fsm_state\": params.get(\"fsm_state\", \"idle\"),\n        \"run_id\": params.get(\"run_id\"),\n        \"target\": params.get(\"target\"),\n        \"plan\": params.get(\"plan\"),\n        \"step_index\": params.get(\"step_index\"),\n        \"step_count\": params.get(\"step_count\"),\n        \"current_step\": params.get(\"current_step\"),\n        \"execution_log\": params.get(\"execution_log\"),\n        \"artifacts\": params.get(\"artifacts\"),\n        \"iteration\": params.get(\"iteration\"),\n        \"score\": params.get(\"score\"),\n        \"feedback\": params.get(\"feedback\"),\n        \"is_satisfied\": params.get(\"is_satisfied\"),\n        \"repair_attempts\": params.get(\"repair_attempts\"),\n        \"step_error_count\": params.get(\"step_error_count\")\n    }\n    try:\n        path.write_text(json.dumps(data, indent=2))\n        return {\"error\": None}\n    except Exception as e:\n        return {\"error\": str(e)}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.load_state", "type": "function", "label": "load_state", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1400, "endLine": 1411, "signature": "(params) -> Dict[]", "docstring": "Load state from run folder.", "source": "def load_state(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Load state from run folder.\"\"\"\n    runDir = Path(params.get(\"run_dir\", params.get(\"workspace_path\", \".\")))\n    path = runDir / \"state.json\"\n    if not path.exists():\n        return {\"has_saved_state\": False}\n    try:\n        data = json.loads(path.read_text())\n        data[\"has_saved_state\"] = True\n        return data\n    except Exception as e:\n        return {\"has_saved_state\": False, \"error\": str(e)}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.clear_state", "type": "function", "label": "clear_state", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1414, "endLine": 1420, "signature": "(params) -> Dict[]", "docstring": "Clear saved state.", "source": "def clear_state(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Clear saved state.\"\"\"\n    runDir = Path(params.get(\"run_dir\", params.get(\"workspace_path\", \".\")))\n    path = runDir / \"state.json\"\n    if path.exists():\n        path.unlink()\n    return {\"error\": None}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.log_corrections", "type": "function", "label": "log_corrections", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1423, "endLine": 1441, "signature": "(params) -> Dict[]", "docstring": "Log auto-corrections for review and auto-approve.", "source": "def log_corrections(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Log auto-corrections for review and auto-approve.\"\"\"\n    corrections = params.get(\"corrections\", [])\n    runDir = Path(params.get(\"run_dir\", \".\"))\n    stepIdx = params.get(\"step_index\", 0)\n\n    if not corrections:\n        return {\"corrections_approved\": True}\n\n    # Format and log corrections report\n    report = _formatCorrectionsReport(corrections)\n    _logStep(runDir, stepIdx, \"AUTO_CORRECTIONS\", report)\n\n    # Print for visibility\n    print(f\"\\n{report}\\n\")\n\n    # Auto-approve all corrections (they've been logged for review)\n    # In future, critical corrections could pause for human approval\n    return {\"corrections_approved\": True}", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.evaluate_interactive", "type": "function", "label": "evaluate_interactive", "direction": "inbound", "parent": "skill_contractor_compute", "line": 1444, "endLine": 1511, "signature": "(params) -> Dict[]", "docstring": "Evaluate generated interactive.py and related Python files.\nUses frame_py.operational_validator for validation.\n\nValidation stages:\n1. Content - LLM-specific errors (literal newlines, placeholders)\n2. Syntax - AST parsing with line numbers\n3. Import - Module reference validation\n4. Structure - Required elements (COMPUTE_UNITS, etc.)\n\nReturns evaluation results with pass/fail status and feedback.", "source": "def evaluate_interactive(params: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"\n    Evaluate generated interactive.py and related Python files.\n    Uses frame_py.operational_validator for validation.\n\n    Validation stages:\n    1. Content - LLM-specific errors (literal newlines, placeholders)\n    2. Syntax - AST parsing with line numbers\n    3. Import - Module reference validation\n    4. Structure - Required elements (COMPUTE_UNITS, etc.)\n\n    Returns evaluation results with pass/fail status and feedback.\n    \"\"\"\n    runDir = Path(params.get(\"run_dir\", \".\"))\n    outputDir = runDir / \"output\"\n\n    _logRun(runDir, \"EVAL_INTERACTIVE: Starting evaluation\")\n\n    if not outputDir.exists():\n        _logRun(runDir, \"EVAL_INTERACTIVE: No output directory found\")\n        return {\n            \"interactive_valid\": True,  # No output = nothing to fail\n            \"interactive_feedback\": \"No output directory found\",\n            \"error\": None\n        }\n\n    # Use the framework's operational validator\n    try:\n        # Try to import from installed package\n        from frame_py.operational_validator import validate_skill_directory\n    except ImportError:\n        # Fall back to relative import path\n        import sys\n        lpp_root = params.get(\"lpp_root\", \"\")\n        if lpp_root:\n            sys.path.insert(0, os.path.join(lpp_root, \"src\"))\n            from frame_py.operational_validator import validate_skill_directory\n        else:\n            # Cannot import validator, skip this check\n            _logRun(runDir, \"EVAL_INTERACTIVE: Validator not available, skipping\")\n            return {\n                \"interactive_valid\": True,\n                \"interactive_feedback\": \"Validator not available\",\n                \"error\": None\n            }\n\n    # Run validation\n    results = validate_skill_directory(str(outputDir), verbose=True)\n\n    # Log results\n    _logRun(\n        runDir, f\"EVAL_INTERACTIVE: {'PASSED' if results['passed'] else 'FAILED'}\")\n    _logRun(runDir, f\"FEEDBACK: {results['feedback'][:500]}\")\n\n    if results[\"passed\"]:\n        print(f\"  [EVAL_INTERACTIVE] \u2713 {results['feedback']}\")\n    else:\n        print(\n            f\"  [EVAL_INTERACTIVE] \u2717 FAILED - {len(results['errors'])} errors\")\n        # Print first few lines of feedback\n        for line in results['feedback'].split('\\n')[:8]:\n            print(f\"    {line}\")\n\n    return {\n        \"interactive_valid\": results[\"passed\"],\n        \"interactive_feedback\": results[\"feedback\"],\n        \"error\": None\n    }", "args": ["params"], "returns": "Dict[]", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "skill_contractor_compute.sanitize_python_code", "type": "function", "label": "sanitize_python_code", "direction": "inbound", "parent": "skill_contractor_compute", "line": 35, "endLine": 36, "signature": "(code, filename, verbose)", "docstring": null, "source": "    def sanitize_python_code(code, filename=\"code.py\", verbose=False):\n        return code, []  # No-op fallback", "args": ["code", "filename", "verbose"], "returns": null, "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "json", "type": "dependency", "label": "json", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "os", "type": "dependency", "label": "os", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "re", "type": "dependency", "label": "re", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "subprocess", "type": "dependency", "label": "subprocess", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "datetime", "type": "dependency", "label": "datetime", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "pathlib", "type": "dependency", "label": "pathlib", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "typing", "type": "dependency", "label": "typing", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "relative.level1", "type": "dependency", "label": "level1", "direction": "outbound", "category": "local", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "openai", "type": "dependency", "label": "openai", "direction": "outbound", "category": "pip", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "frame_py.operational_validator", "type": "dependency", "label": "operational_validator", "direction": "outbound", "category": "pip", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}, {"id": "sys", "type": "dependency", "label": "sys", "direction": "outbound", "category": "stdlib", "moduleColor": "#00d4ff", "moduleName": "skill_contractor_compute"}];
const edges = [{"from": "skill_contractor_compute.write_output", "to": "skill_contractor_compute.sanitize_python_code", "type": "internal", "line": 994}, {"from": "skill_contractor_compute._getRunDir", "to": "pathlib", "type": "external", "category": "stdlib", "line": 45}, {"from": "skill_contractor_compute._getRunDir", "to": "datetime", "type": "external", "category": "stdlib", "line": 52}, {"from": "skill_contractor_compute._logStep", "to": "datetime", "type": "external", "category": "stdlib", "line": 62}, {"from": "skill_contractor_compute._logRun", "to": "datetime", "type": "external", "category": "stdlib", "line": 90}, {"from": "skill_contractor_compute._condenseForEval", "to": "pathlib", "type": "external", "category": "stdlib", "line": 148}, {"from": "skill_contractor_compute._extractJson", "to": "json", "type": "external", "category": "stdlib", "line": 198}, {"from": "skill_contractor_compute._extractJson", "to": "json", "type": "external", "category": "stdlib", "line": 205}, {"from": "skill_contractor_compute._extractJson", "to": "re", "type": "external", "category": "stdlib", "line": 213}, {"from": "skill_contractor_compute._extractJson", "to": "re", "type": "external", "category": "stdlib", "line": 214}, {"from": "skill_contractor_compute._extractJson", "to": "json", "type": "external", "category": "stdlib", "line": 216}, {"from": "skill_contractor_compute._extractJson", "to": "json", "type": "external", "category": "stdlib", "line": 221}, {"from": "skill_contractor_compute._callLlm", "to": "openai", "type": "external", "category": "pip", "line": 236}, {"from": "skill_contractor_compute._findLppRoot", "to": "os", "type": "external", "category": "stdlib", "line": 248}, {"from": "skill_contractor_compute._findLppRoot", "to": "pathlib", "type": "external", "category": "stdlib", "line": 251}, {"from": "skill_contractor_compute._findLppRoot", "to": "os", "type": "external", "category": "stdlib", "line": 251}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 546}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 546}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 558}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 559}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 560}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 567}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 568}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 569}, {"from": "skill_contractor_compute.init", "to": "os", "type": "external", "category": "stdlib", "line": 570}, {"from": "skill_contractor_compute.detect_lpp_target", "to": "pathlib", "type": "external", "category": "stdlib", "line": 596}, {"from": "skill_contractor_compute.decompose", "to": "pathlib", "type": "external", "category": "stdlib", "line": 615}, {"from": "skill_contractor_compute.generate_step_output", "to": "pathlib", "type": "external", "category": "stdlib", "line": 707}, {"from": "skill_contractor_compute.parse_and_sanitize", "to": "pathlib", "type": "external", "category": "stdlib", "line": 851}, {"from": "skill_contractor_compute.parse_and_sanitize", "to": "json", "type": "external", "category": "stdlib", "line": 884}, {"from": "skill_contractor_compute.write_output", "to": "pathlib", "type": "external", "category": "stdlib", "line": 944}, {"from": "skill_contractor_compute.write_output", "to": "json", "type": "external", "category": "stdlib", "line": 971}, {"from": "skill_contractor_compute.write_output", "to": "frame_py", "type": "external", "category": "pip", "line": 994}, {"from": "skill_contractor_compute.write_output", "to": "subprocess", "type": "external", "category": "stdlib", "line": 1010}, {"from": "skill_contractor_compute.write_output", "to": "json", "type": "external", "category": "stdlib", "line": 1032}, {"from": "skill_contractor_compute.advance_step", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1048}, {"from": "skill_contractor_compute.validate_lpp", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1060}, {"from": "skill_contractor_compute.validate_lpp", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1072}, {"from": "skill_contractor_compute.validate_lpp", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1080}, {"from": "skill_contractor_compute.validate_lpp", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1081}, {"from": "skill_contractor_compute.validate_lpp", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1091}, {"from": "skill_contractor_compute.validate_lpp", "to": "subprocess", "type": "external", "category": "stdlib", "line": 1095}, {"from": "skill_contractor_compute.advance_phase", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1140}, {"from": "skill_contractor_compute.evaluate", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1166}, {"from": "skill_contractor_compute.evaluate", "to": "json", "type": "external", "category": "stdlib", "line": 1183}, {"from": "skill_contractor_compute.capture_error", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1237}, {"from": "skill_contractor_compute.capture_step_error", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1253}, {"from": "skill_contractor_compute.capture_parse_error", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1269}, {"from": "skill_contractor_compute.review_failed_step", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1303}, {"from": "skill_contractor_compute.review_failed_step", "to": "json", "type": "external", "category": "stdlib", "line": 1318}, {"from": "skill_contractor_compute.skip_step", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1359}, {"from": "skill_contractor_compute.save_state", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1373}, {"from": "skill_contractor_compute.save_state", "to": "json", "type": "external", "category": "stdlib", "line": 1394}, {"from": "skill_contractor_compute.load_state", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1402}, {"from": "skill_contractor_compute.load_state", "to": "json", "type": "external", "category": "stdlib", "line": 1407}, {"from": "skill_contractor_compute.clear_state", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1416}, {"from": "skill_contractor_compute.log_corrections", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1426}, {"from": "skill_contractor_compute.evaluate_interactive", "to": "pathlib", "type": "external", "category": "stdlib", "line": 1457}, {"from": "skill_contractor_compute.evaluate_interactive", "to": "sys", "type": "external", "category": "stdlib", "line": 1479}, {"from": "skill_contractor_compute.evaluate_interactive", "to": "os", "type": "external", "category": "stdlib", "line": 1479}, {"from": "skill_contractor_compute.evaluate_interactive", "to": "frame_py", "type": "external", "category": "pip", "line": 1491}, {"from": "skill_contractor_compute.decompose", "to": "prompts", "type": "local", "line": 653}, {"from": "skill_contractor_compute.generate_step_output", "to": "prompts", "type": "local", "line": 804}, {"from": "skill_contractor_compute.evaluate", "to": "prompts", "type": "local", "line": 1175}, {"from": "skill_contractor_compute.review_failed_step", "to": "prompts", "type": "local", "line": 1312}];
const moduleColors = {"skill_contractor_compute": "#00d4ff"};

// Edge type visibility
const edgeVisibility = { internal: true, external: true, local: true };
let currentLayout = 'force';

// Create node lookup
const nodeById = {};
nodes.forEach(n => nodeById[n.id] = n);

// Setup SVG
const container = document.getElementById('graph');
const width = container.clientWidth;
const height = container.clientHeight || 600;

const svg = d3.select("svg").attr("viewBox", [0, 0, width, height]);
const g = svg.append("g");

// Zoom behavior
const zoom = d3.zoom()
    .scaleExtent([0.1, 4])
    .filter(e => !e.target.closest('.node'))
    .on("zoom", e => g.attr("transform", e.transform));
svg.call(zoom);

// Arrow markers
const defs = svg.append("defs");
["internal", "external", "local"].forEach(type => {
    const color = type === "internal" ? "#4ecdc4" : type === "external" ? "#f39c12" : "#9b59b6";
    defs.append("marker")
        .attr("id", `arrow-${type}`)
        .attr("viewBox", "0 -5 10 10")
        .attr("refX", 20)
        .attr("refY", 0)
        .attr("markerWidth", 6)
        .attr("markerHeight", 6)
        .attr("orient", "auto")
        .append("path")
        .attr("d", "M0,-4L10,0L0,4")
        .attr("fill", color);
});

// Build module legend
const legendDiv = document.getElementById('module-legend');
Object.entries(moduleColors).forEach(([mod, color]) => {
    const item = document.createElement('div');
    item.className = 'module-item';
    item.innerHTML = `<div class="module-dot" style="background:${color}"></div><span class="module-name">${mod}</span>`;
    item.onclick = () => highlightModule(mod);
    legendDiv.appendChild(item);
});

// Process edges - resolve node references
const processedEdges = edges.map(e => ({
    ...e,
    source: nodeById[e.from] || { id: e.from, x: 0, y: 0 },
    target: nodeById[e.to] || { id: e.to, x: 0, y: 0 }
})).filter(e => e.source && e.target);

// Force simulation
const simulation = d3.forceSimulation(nodes)
    .force("link", d3.forceLink(processedEdges).id(d => d.id).distance(80).strength(0.5))
    .force("charge", d3.forceManyBody().strength(-300))
    .force("center", d3.forceCenter(width / 2, height / 2))
    .force("collision", d3.forceCollide().radius(40));

// Draw edges
const edge = g.append("g").selectAll("path")
    .data(processedEdges)
    .join("path")
    .attr("class", d => `edge edge-${d.type || 'internal'}`)
    .attr("marker-end", d => `url(#arrow-${d.type || 'internal'})`);

// Node size based on type
function nodeSize(d) {
    if (d.type === 'module') return { w: 120, h: 40 };
    if (d.type === 'function' || d.type === 'async_function') return { w: 100, h: 30 };
    if (d.type === 'class') return { w: 110, h: 35 };
    return { w: 80, h: 25 };  // dependency
}

// Draw nodes
const node = g.append("g").selectAll("g")
    .data(nodes)
    .join("g")
    .attr("class", "node")
    .call(d3.drag()
        .on("start", dragStart)
        .on("drag", dragging)
        .on("end", dragEnd));

node.append("rect")
    .attr("class", d => `node-${d.type === 'dependency' ? 'dependency' : d.type === 'module' ? 'module' : 'function'}`)
    .attr("width", d => nodeSize(d).w)
    .attr("height", d => nodeSize(d).h)
    .attr("x", d => -nodeSize(d).w / 2)
    .attr("y", d => -nodeSize(d).h / 2)
    .attr("rx", 6)
    .attr("stroke", d => d.moduleColor || "#666");

node.append("text")
    .attr("class", "node-label")
    .attr("text-anchor", "middle")
    .attr("dy", d => d.signature ? -3 : 4)
    .text(d => d.label || d.id);

node.filter(d => d.signature).append("text")
    .attr("class", "node-sublabel")
    .attr("text-anchor", "middle")
    .attr("dy", 10)
    .text(d => d.signature.length > 20 ? d.signature.slice(0, 18) + ".." : d.signature);

// Tooltip
const tooltip = d3.select("#tooltip");
node.on("mouseover", (e, d) => {
    let html = `<b>${d.label || d.id}</b>`;
    if (d.type) html += `<br><span style="color:#888">${d.type}</span>`;
    if (d.signature) html += `<br><code>${d.signature}</code>`;
    if (d.direction) html += `<br>Direction: ${d.direction}`;
    tooltip.style("display", "block").html(html);
})
.on("mousemove", e => {
    tooltip.style("left", (e.pageX + 15) + "px").style("top", (e.pageY - 10) + "px");
})
.on("mouseout", () => tooltip.style("display", "none"));

// Click to select
node.on("click", (e, d) => {
    e.stopPropagation();
    selectNode(d);
});

svg.on("click", () => clearSelection());

// Update positions
simulation.on("tick", () => {
    edge.attr("d", d => {
        if (!d.source || !d.target) return "";
        const dx = d.target.x - d.source.x;
        const dy = d.target.y - d.source.y;
        const dr = Math.sqrt(dx * dx + dy * dy) * 1.5;
        return `M${d.source.x},${d.source.y}A${dr},${dr} 0 0,1 ${d.target.x},${d.target.y}`;
    });
    node.attr("transform", d => `translate(${d.x},${d.y})`);
});

// Drag functions
function dragStart(e, d) {
    if (!e.active) simulation.alphaTarget(0.3).restart();
    d.fx = d.x;
    d.fy = d.y;
}
function dragging(e, d) {
    d.fx = e.x;
    d.fy = e.y;
}
function dragEnd(e, d) {
    if (!e.active) simulation.alphaTarget(0);
    d.fx = null;
    d.fy = null;
}

// Selection functions
let selectedNode = null;

function selectNode(d) {
    selectedNode = d;
    
    // Highlight node
    node.classed("dim", n => n.id !== d.id && !isConnected(d, n));
    node.select("rect").attr("stroke-width", n => n.id === d.id ? 4 : 2);
    
    // Highlight edges
    edge.classed("dim", e => e.source.id !== d.id && e.target.id !== d.id);
    edge.classed("highlight", e => e.source.id === d.id || e.target.id === d.id);
    
    // Update info panel
    updateNodeInfo(d);
    updateEdgeList(d);
}

function clearSelection() {
    selectedNode = null;
    node.classed("dim", false);
    node.select("rect").attr("stroke-width", 2);
    edge.classed("dim", false).classed("highlight", false);
    document.getElementById('node-info').innerHTML = 'Click a node to see details';
    document.getElementById('edge-list').innerHTML = '';
}

function isConnected(a, b) {
    return processedEdges.some(e => 
        (e.source.id === a.id && e.target.id === b.id) ||
        (e.source.id === b.id && e.target.id === a.id)
    );
}

function updateNodeInfo(d) {
    let html = `<div class="metric"><span class="info-label">ID:</span><span class="info-value">${d.id}</span></div>`;
    html += `<div class="metric"><span class="info-label">Type:</span><span class="info-value">${d.type}</span></div>`;
    if (d.moduleName) html += `<div class="metric"><span class="info-label">Module:</span><span class="info-value">${d.moduleName}</span></div>`;
    if (d.line) html += `<div class="metric"><span class="info-label">Line:</span><span class="info-value">${d.line}${d.endLine ? '-' + d.endLine : ''}</span></div>`;
    if (d.signature) html += `<div class="metric"><span class="info-label">Signature:</span><span class="info-value" style="font-family:monospace">${d.signature}</span></div>`;
    if (d.direction) html += `<div class="metric"><span class="info-label">Direction:</span><span class="info-value">${d.direction}</span></div>`;
    if (d.category) html += `<div class="metric"><span class="info-label">Category:</span><span class="info-value">${d.category}</span></div>`;

    if (d.metrics) {
        html += `<div style="margin-top:10px"><b>Coupling Metrics</b></div>`;
        html += `<div class="metric"><span class="info-label">Fan-In:</span><span class="info-value">${d.metrics.fanIn}</span></div>`;
        html += `<div class="metric"><span class="info-label">Fan-Out:</span><span class="info-value">${d.metrics.fanOut}</span></div>`;
        html += `<div class="metric"><span class="info-label">Instability:</span><span class="info-value">${(d.metrics.instability * 100).toFixed(1)}%</span></div>`;
        html += `<div class="metric-bar"><div class="metric-fill" style="width:${d.metrics.instability * 100}%;background:${d.metrics.instability > 0.5 ? '#ff6b6b' : '#4ecdc4'}"></div></div>`;
        html += `<div class="metric"><span class="info-label">Internal Edges:</span><span class="info-value">${d.metrics.internalEdges}</span></div>`;
    }

    document.getElementById('node-info').innerHTML = html;

    // Update source code panel
    updateSourcePanel(d);
}

function escapeHtml(str) {
    if (!str) return '';
    return str.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');
}

function updateSourcePanel(d) {
    const panel = document.getElementById('source-content');

    if (d.source) {
        let sourceHtml = '';
        if (d.docstring) {
            sourceHtml += `<div class="docstring" style="margin-bottom:8px;padding:5px;background:#1a1a2a;border-radius:3px">${escapeHtml(d.docstring)}</div>`;
        }
        sourceHtml += `<div class="source-code">${escapeHtml(d.source)}</div>`;
        panel.innerHTML = sourceHtml;
    } else if (d.type === 'module') {
        panel.innerHTML = `<div style="color:#888;font-size:11px">Module: ${d.label}<br>Click a function to view its source code.</div>`;
    } else if (d.type === 'dependency') {
        panel.innerHTML = `<div style="color:#888;font-size:11px">External dependency: ${d.label}<br>Category: ${d.category || 'unknown'}</div>`;
    } else {
        panel.innerHTML = `<div style="color:#666;font-size:11px">No source available for this node</div>`;
    }
}

function updateEdgeList(d) {
    const outgoing = processedEdges.filter(e => e.source.id === d.id);
    const incoming = processedEdges.filter(e => e.target.id === d.id);
    
    let html = '';
    if (outgoing.length) {
        html += '<div style="color:#4ecdc4;font-weight:bold;margin-bottom:5px">Outgoing →</div>';
        outgoing.forEach(e => {
            html += `<div class="edge-item">→ <span class="to">${e.target.id || e.to}</span> <span class="type">[${e.type}]</span></div>`;
        });
    }
    if (incoming.length) {
        html += '<div style="color:#f39c12;font-weight:bold;margin:10px 0 5px 0">← Incoming</div>';
        incoming.forEach(e => {
            html += `<div class="edge-item">← <span class="from">${e.source.id || e.from}</span> <span class="type">[${e.type}]</span></div>`;
        });
    }
    if (!outgoing.length && !incoming.length) {
        html = '<div style="color:#666">No connections</div>';
    }
    
    document.getElementById('edge-list').innerHTML = html;
}

function highlightModule(modName) {
    node.classed("dim", n => n.moduleName !== modName && n.type !== 'dependency');
    edge.classed("dim", e => {
        const srcMod = nodeById[e.source.id]?.moduleName;
        const tgtMod = nodeById[e.target.id]?.moduleName;
        return srcMod !== modName && tgtMod !== modName;
    });
}

// Layout functions
function toggleLayout(layout) {
    currentLayout = layout;
    document.querySelectorAll('.controls button').forEach(b => {
        if (b.id.startsWith('btn-') && ['force', 'horizontal', 'vertical'].includes(b.id.replace('btn-', ''))) {
            b.classList.toggle('active', b.id === `btn-${layout}`);
        }
    });
    
    if (layout === 'force') {
        simulation.alpha(1).restart();
    } else {
        simulation.stop();
        layoutNodes(layout);
    }
}

function layoutNodes(layout) {
    const modules = [...new Set(nodes.filter(n => n.type === 'module').map(n => n.id))];
    const padding = 50;
    
    if (layout === 'horizontal') {
        // Group by module, spread horizontally
        modules.forEach((mod, mi) => {
            const modNodes = nodes.filter(n => n.moduleName === mod || n.id === mod);
            const x = padding + mi * (width - padding * 2) / Math.max(modules.length - 1, 1);
            modNodes.forEach((n, ni) => {
                n.x = x;
                n.y = padding + ni * 50;
            });
        });
        // Dependencies on the right
        const deps = nodes.filter(n => n.type === 'dependency');
        deps.forEach((n, i) => {
            n.x = width - padding;
            n.y = padding + i * 40;
        });
    } else if (layout === 'vertical') {
        // Modules at top, functions below, dependencies at bottom
        const modNodes = nodes.filter(n => n.type === 'module');
        const funcNodes = nodes.filter(n => n.type === 'function' || n.type === 'async_function' || n.type === 'class');
        const depNodes = nodes.filter(n => n.type === 'dependency');
        
        modNodes.forEach((n, i) => {
            n.x = padding + i * 150;
            n.y = padding;
        });
        funcNodes.forEach((n, i) => {
            n.x = padding + (i % 6) * 130;
            n.y = 120 + Math.floor(i / 6) * 60;
        });
        depNodes.forEach((n, i) => {
            n.x = padding + (i % 8) * 100;
            n.y = height - padding - 50;
        });
    }
    
    // Update positions
    node.transition().duration(500).attr("transform", d => `translate(${d.x},${d.y})`);
    edge.transition().duration(500).attr("d", d => {
        if (!d.source || !d.target) return "";
        const dx = d.target.x - d.source.x;
        const dy = d.target.y - d.source.y;
        const dr = Math.sqrt(dx * dx + dy * dy) * 1.5;
        return `M${d.source.x},${d.source.y}A${dr},${dr} 0 0,1 ${d.target.x},${d.target.y}`;
    });
}

// Edge type toggle
function toggleEdgeType(type) {
    edgeVisibility[type] = !edgeVisibility[type];
    document.getElementById(`btn-${type}`).classList.toggle('active', edgeVisibility[type]);
    edge.style("display", d => edgeVisibility[d.type || 'internal'] ? null : "none");
}

// View controls
function resetView() {
    svg.transition().duration(500).call(zoom.transform, d3.zoomIdentity);
}

function fitToView() {
    const bounds = g.node().getBBox();
    const scale = Math.min(width / bounds.width, height / bounds.height) * 0.9;
    const tx = (width - bounds.width * scale) / 2 - bounds.x * scale;
    const ty = (height - bounds.height * scale) / 2 - bounds.y * scale;
    svg.transition().duration(500).call(zoom.transform, d3.zoomIdentity.translate(tx, ty).scale(scale));
}

// Initial fit
setTimeout(fitToView, 1000);
</script>
</body>
</html>