#!/usr/bin/env python3
"""
Comprehensive Test Generator for L++ Blueprints.

Generates both:
1. Blueprint Tests (Infrastructure) - State/transition coverage
2. Compute Tests (Features) - Python unit test coverage

Usage:
    python comprehensive_test.py <blueprint.json> [--output-dir <dir>]
    python comprehensive_test.py --all [--workflow-dir <dir>]
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# Add project root to path for imports
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from utils.test_generator.src.test_compute import (
    load_blueprint,
    build_graph,
    analyze_paths,
    analyze_gates,
    generate_path_tests,
    generate_state_tests,
    generate_gate_tests,
    generate_negative_tests,
    generate_property_tests,
    generate_contract_tests,
    combine_tests,
    format_pytest,
)


# =============================================================================
# Compute Function Discovery
# =============================================================================

def discover_compute_units(blueprint: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Extract all compute units referenced in a blueprint.

    Returns list of:
    {
        "unit_id": "lvp:generate_patches",
        "action_id": "generate_patches",
        "input_map": {...},
        "output_map": {...}
    }
    """
    compute_units = []

    actions = blueprint.get("actions", {})
    for action_id, action in actions.items():
        if action.get("type") == "compute":
            compute_units.append({
                "unit_id": action.get("compute_unit", action_id),
                "action_id": action_id,
                "input_map": action.get("input_map", {}),
                "output_map": action.get("output_map", {}),
            })

    return compute_units


def generate_compute_unit_tests(
    blueprint: Dict[str, Any],
    compute_units: List[Dict[str, Any]],
    compute_module_path: Optional[str] = None
) -> str:
    """
    Generate pytest tests for compute unit functions.

    These tests verify:
    1. Input validation - compute handles missing/invalid inputs
    2. Output contract - compute returns expected fields
    3. Error handling - compute handles errors gracefully
    """
    bp_id = blueprint.get("id", "unknown")

    # Determine import path for compute module
    if compute_module_path:
        # Convert file path to module path
        import_path = _file_to_module_path(compute_module_path)
    else:
        import_path = None

    # Collect unique function names for imports
    func_imports = {}
    for cu in compute_units:
        unit_id = cu["unit_id"]
        if ":" in unit_id:
            module_prefix, func_name = unit_id.split(":", 1)
        else:
            func_name = unit_id
        safe_func_name = _sanitize_name(func_name)
        func_imports[safe_func_name] = func_name

    lines = [
        '"""',
        f'Compute Unit Tests for {bp_id}',
        '',
        'Tests for the Python compute functions (feature implementation).',
        'Generated by L++ Comprehensive Test Generator.',
        '"""',
        '',
        'import pytest',
        'import sys',
        'from pathlib import Path',
        'from typing import Any, Dict',
        '',
        '# Add project root to path',
        'PROJECT_ROOT = Path(__file__).parent.parent.parent',
        'sys.path.insert(0, str(PROJECT_ROOT))',
        '',
    ]

    # Add import for the compute module if we found it
    if import_path:
        func_list = ', '.join(func_imports.values())
        lines.extend([
            f'# Import compute functions from discovered module',
            f'try:',
            f'    from {import_path} import {func_list}',
            f'    COMPUTE_AVAILABLE = True',
            f'except ImportError as e:',
            f'    COMPUTE_AVAILABLE = False',
            f'    IMPORT_ERROR = str(e)',
            '',
        ])
    else:
        lines.extend([
            '# No compute module discovered - tests will skip actual calls',
            'COMPUTE_AVAILABLE = False',
            'IMPORT_ERROR = "Compute module not found"',
            '',
        ])

    lines.extend([
        '',
        '# =============================================================================',
        '# Compute Unit Test Fixtures',
        '# =============================================================================',
        '',
        '@pytest.fixture',
        'def mock_context():',
        '    """Create a mock context with default values."""',
        '    return {',
    ])

    # Generate default context from schema
    context_schema = blueprint.get("context_schema", {})
    properties = context_schema.get("properties", {})

    for prop_name, prop_spec in properties.items():
        prop_type = prop_spec.get("type", "string")
        default_val = _get_default_value(prop_type)
        lines.append(f'        "{prop_name}": {default_val},')

    lines.extend([
        '    }',
        '',
        '',
    ])

    # Generate tests for each compute unit
    for idx, cu in enumerate(compute_units, 1):
        unit_id = cu["unit_id"]
        action_id = cu["action_id"]
        input_map = cu["input_map"]
        output_map = cu["output_map"]

        # Parse module and function from unit_id (e.g., "lvp:generate_patches")
        if ":" in unit_id:
            module_prefix, func_name = unit_id.split(":", 1)
        else:
            module_prefix = "unknown"
            func_name = unit_id

        # Sanitize func_name for use in Python identifiers
        safe_func_name = _sanitize_name(func_name)

        lines.extend([
            '# =============================================================================',
            f'# Tests for compute unit: {unit_id}',
            '# =============================================================================',
            '',
            f'class TestCompute_{_to_class_name(func_name)}:',
            f'    """Tests for {unit_id} compute function."""',
            '',
            f'    def test_{safe_func_name}_input_contract(self, mock_context):',
            f'        """Test that {func_name} validates required inputs."""',
            '        inputs = {',
        ])

        for inp_key, ctx_key in input_map.items():
            lines.append(f'            "{inp_key}": mock_context.get("{ctx_key}"),')

        lines.extend([
            '        }',
            '',
            '        # Verify inputs structure',
            '        assert inputs is not None',
        ])

        for inp_key in input_map.keys():
            lines.append(f'        assert "{inp_key}" in inputs')

        # Add actual function call if available
        lines.extend([
            '',
            '        # Call actual compute function if available',
            '        if COMPUTE_AVAILABLE:',
            f'            result = {func_name}(inputs)',
            '            assert result is not None',
            '            assert isinstance(result, dict)',
            '',
        ])

        # Output contract test
        # Note: output_map format is {"context_field": "source_field"}
        # We check for source_field names since that's what the function returns
        lines.extend([
            f'    def test_{safe_func_name}_output_contract(self, mock_context):',
            f'        """Test that {func_name} returns expected output fields."""',
            f'        expected_outputs = {list(output_map.values())}',
            '',
            '        if not COMPUTE_AVAILABLE:',
            '            pytest.skip(f"Compute module not available: {IMPORT_ERROR}")',
            '',
            '        inputs = {',
        ])

        for inp_key, ctx_key in input_map.items():
            lines.append(f'            "{inp_key}": mock_context.get("{ctx_key}"),')

        lines.extend([
            '        }',
            '',
            f'        result = {func_name}(inputs)',
            '        assert result is not None',
            '',
            '        # Verify expected outputs are present',
            '        for field in expected_outputs:',
            '            assert field in result, f"Missing output field: {field}"',
            '',
        ])

        # Error handling test
        lines.extend([
            f'    def test_{safe_func_name}_error_handling(self, mock_context):',
            f'        """Test that {func_name} handles errors gracefully."""',
            '        if not COMPUTE_AVAILABLE:',
            '            pytest.skip(f"Compute module not available: {IMPORT_ERROR}")',
            '',
            '        # Test with empty inputs',
            '        invalid_inputs = {}',
            '',
            '        try:',
            f'            result = {func_name}(invalid_inputs)',
            '            # Should either return error dict or handle gracefully',
            '            if result is not None:',
            '                assert isinstance(result, dict)',
            '        except (KeyError, TypeError, ValueError):',
            '            # Expected - function requires inputs',
            '            pass',
            '',
        ])

        # Generate null input tests
        for inp_key, ctx_key in input_map.items():
            prop_spec = properties.get(ctx_key, {})
            safe_inp_key = _sanitize_name(inp_key)

            lines.extend([
                f'    def test_{safe_func_name}_{safe_inp_key}_null(self, mock_context):',
                f'        """Test {func_name} with null {inp_key}."""',
                '        if not COMPUTE_AVAILABLE:',
                '            pytest.skip(f"Compute module not available: {IMPORT_ERROR}")',
                '',
                '        inputs = {',
            ])

            for ik, ck in input_map.items():
                if ik == inp_key:
                    lines.append(f'            "{ik}": None,')
                else:
                    lines.append(f'            "{ik}": mock_context.get("{ck}"),')

            lines.extend([
                '        }',
                '',
                '        try:',
                f'            result = {func_name}(inputs)',
                '            # Function should handle null gracefully',
                '            assert result is not None',
                '        except (KeyError, TypeError, ValueError, AttributeError):',
                f'            # Expected - {inp_key} is required',
                '            pass',
                '',
            ])

        lines.append('')

    return '\n'.join(lines)


def _file_to_module_path(file_path: str) -> str:
    """Convert a file path to a Python module import path."""
    path = Path(file_path)
    # Remove .py extension
    if path.suffix == '.py':
        path = path.with_suffix('')
    # Convert path to module format
    parts = path.parts
    # Find src or workflows directory as base
    for i, part in enumerate(parts):
        if part in ('src', 'workflows', 'utils'):
            return '.'.join(parts[i:])
    # Fallback - use last 3 parts
    return '.'.join(parts[-3:])


def _discover_compute_module(blueprint_path: str, bp_id: str) -> Optional[str]:
    """
    Discover the compute module for a blueprint.

    Searches for *_compute.py files near the blueprint.
    """
    bp_path = Path(blueprint_path)
    bp_dir = bp_path.parent

    # Common patterns for compute modules
    search_patterns = [
        # Same directory
        bp_dir / f"{bp_id}_compute.py",
        bp_dir / f"{bp_id.replace('_', '')}_compute.py",
        bp_dir / "compute.py",
        # src subdirectory
        bp_dir / "src" / f"{bp_id}_compute.py",
        bp_dir / "src" / f"{bp_id.replace('_', '')}_compute.py",
        bp_dir / "src" / "compute.py",
        # Parent src directory
        bp_dir.parent / "src" / f"{bp_id}_compute.py",
        bp_dir.parent / "src" / f"{bp_id.replace('lvp_', 'lvp_')}_compute.py",
        bp_dir.parent / "src" / "lvp_compute.py",
        # Components pattern (for assemblies)
        bp_dir.parent / "src" / f"{bp_dir.parent.name}_compute.py",
    ]

    # Try each pattern
    for pattern in search_patterns:
        if pattern.exists():
            return str(pattern)

    # Search recursively in parent directories for *_compute.py
    search_dir = bp_dir
    for _ in range(3):  # Go up 3 levels max
        for compute_file in search_dir.glob("**/*_compute.py"):
            # Skip __pycache__
            if "__pycache__" in str(compute_file):
                continue
            return str(compute_file)
        search_dir = search_dir.parent

    return None


def _get_default_value(prop_type: str) -> str:
    """Get Python default value for a schema type."""
    defaults = {
        "string": '""',
        "number": "0",
        "integer": "0",
        "boolean": "False",
        "array": "[]",
        "object": "{}",
    }
    return defaults.get(prop_type, "None")


def _sanitize_name(name: str) -> str:
    """Sanitize a name for use in Python identifiers."""
    # Replace dots and other invalid characters with underscores
    result = ''.join(c if c.isalnum() or c == '_' else '_' for c in name)
    # Remove leading digits
    while result and result[0].isdigit():
        result = result[1:]
    return result or 'unnamed'


def _to_class_name(name: str) -> str:
    """Convert function name to PascalCase class name."""
    name = _sanitize_name(name)
    return ''.join(word.capitalize() for word in name.split('_'))


# =============================================================================
# Unified Test Generation
# =============================================================================

def generate_comprehensive_tests(
    blueprint_path: str,
    output_dir: Optional[str] = None
) -> Dict[str, Any]:
    """
    Generate comprehensive tests for a blueprint.

    Returns:
    {
        "blueprint_tests": {...},
        "compute_tests": {...},
        "coverage": {...}
    }
    """
    # Load blueprint
    result = load_blueprint({"path": blueprint_path})
    if result["error"]:
        return {"error": result["error"]}

    bp = result["blueprint"]
    bp_id = bp.get("id", "unknown")

    # Output directory
    if output_dir is None:
        output_dir = Path(blueprint_path).parent / "tests" / "generated"
    else:
        output_dir = Path(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)

    # =========================================================================
    # 1. Generate Blueprint Tests (Infrastructure)
    # =========================================================================
    graph_result = build_graph({"blueprint": bp})
    graph = graph_result["graph"]

    paths = analyze_paths({"blueprint": bp, "graph": graph})["paths"]
    gate_analysis = analyze_gates({"blueprint": bp})["analysis"]

    path_tests = generate_path_tests({"blueprint": bp, "paths": paths})["tests"]
    state_tests = generate_state_tests({"blueprint": bp, "paths": paths})["tests"]
    gate_tests = generate_gate_tests({"blueprint": bp, "gate_analysis": gate_analysis})["tests"]
    negative_tests = generate_negative_tests({"blueprint": bp, "graph": graph})["tests"]
    property_tests = generate_property_tests({"blueprint": bp})["tests"]
    contract_tests = generate_contract_tests({"blueprint": bp, "graph": graph_result})["tests"]

    combined = combine_tests({
        "blueprint": bp,
        "path_tests": path_tests,
        "state_tests": state_tests,
        "gate_tests": gate_tests,
        "negative_tests": negative_tests,
        "property_tests": property_tests,
        "contract_tests": contract_tests,
    })

    # Format as pytest
    blueprint_pytest = format_pytest({
        "blueprint": bp,
        "tests": combined["tests"]
    })["output"]

    # Write blueprint tests
    bp_test_file = output_dir / f"test_{bp_id}_blueprint.py"
    with open(bp_test_file, "w") as f:
        f.write(blueprint_pytest)

    # =========================================================================
    # 2. Generate Compute Tests (Features)
    # =========================================================================
    compute_units = discover_compute_units(bp)

    if compute_units:
        # Discover compute module path
        compute_module_path = _discover_compute_module(blueprint_path, bp_id)
        compute_pytest = generate_compute_unit_tests(bp, compute_units, compute_module_path)

        # Write compute tests
        compute_test_file = output_dir / f"test_{bp_id}_compute.py"
        with open(compute_test_file, "w") as f:
            f.write(compute_pytest)
    else:
        compute_pytest = None
        compute_test_file = None

    # =========================================================================
    # 3. Generate Coverage Summary
    # =========================================================================
    coverage = combined.get("coverage", {})

    summary = {
        "blueprint_id": bp_id,
        "blueprint_tests": {
            "file": str(bp_test_file),
            "count": len(combined["tests"]),
            "coverage": coverage,
        },
        "compute_tests": {
            "file": str(compute_test_file) if compute_test_file else None,
            "units": len(compute_units),
            "units_list": [cu["unit_id"] for cu in compute_units],
        },
        "total_tests": len(combined["tests"]) + (len(compute_units) * 4 if compute_units else 0),
    }

    # Write summary
    summary_file = output_dir / f"{bp_id}_test_summary.json"
    with open(summary_file, "w") as f:
        json.dump(summary, f, indent=2)

    return {
        "summary": summary,
        "blueprint_tests_file": str(bp_test_file),
        "compute_tests_file": str(compute_test_file) if compute_test_file else None,
        "summary_file": str(summary_file),
    }


def generate_all_tests(workflow_dir: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
    """
    Generate tests for all blueprints in a workflow directory.
    """
    workflow_path = Path(workflow_dir)
    results = []

    # Find all blueprint JSON files
    for bp_file in workflow_path.rglob("*.json"):
        # Skip non-blueprint files
        try:
            with open(bp_file) as f:
                data = json.load(f)
            if "states" not in data:
                continue
        except:
            continue

        print(f"Generating tests for: {bp_file}")
        result = generate_comprehensive_tests(
            str(bp_file),
            output_dir
        )

        if "error" in result:
            print(f"  Error: {result['error']}")
        else:
            summary = result.get("summary", {})
            print(f"  Blueprint tests: {summary.get('blueprint_tests', {}).get('count', 0)}")
            print(f"  Compute units: {summary.get('compute_tests', {}).get('units', 0)}")

        results.append(result)

    return {
        "blueprints_processed": len(results),
        "results": results,
    }


# =============================================================================
# CLI
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Generate comprehensive tests for L++ blueprints"
    )
    parser.add_argument(
        "blueprint",
        nargs="?",
        help="Path to blueprint JSON file"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="Generate tests for all blueprints in workflow directory"
    )
    parser.add_argument(
        "--workflow-dir",
        default="workflows",
        help="Workflow directory (default: workflows)"
    )
    parser.add_argument(
        "--output-dir",
        help="Output directory for generated tests"
    )

    args = parser.parse_args()

    if args.all:
        result = generate_all_tests(args.workflow_dir, args.output_dir)
        print(f"\nProcessed {result['blueprints_processed']} blueprints")
    elif args.blueprint:
        result = generate_comprehensive_tests(args.blueprint, args.output_dir)
        if "error" in result:
            print(f"Error: {result['error']}")
            sys.exit(1)
        print(f"\nGenerated tests:")
        print(f"  Blueprint: {result['blueprint_tests_file']}")
        if result['compute_tests_file']:
            print(f"  Compute: {result['compute_tests_file']}")
        print(f"  Summary: {result['summary_file']}")
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == "__main__":
    main()
