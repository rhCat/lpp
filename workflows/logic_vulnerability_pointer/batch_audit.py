#!/usr/bin/env python3
"""
LVP Batch Security Audit
Run LVP on all L++ utilities and workflows
"""

import json
import os
import sys
from datetime import datetime
from pathlib import Path

# Add L++ to path
LPP_ROOT = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(LPP_ROOT))

from workflows.logic_vulnerability_pointer.src.lvp_compute import (
    extract_logic, _generate_heuristic_invariants,
    generate_unittests, generate_llm_unittests,
    analyze_coverage, generate_coverage_tests
)


def audit_file(target_path: str, output_base: str) -> dict:
    """Run LVP audit on a single file."""
    target_name = Path(target_path).stem

    # Skip __init__.py files
    if target_name == "__init__":
        return None

    result = {
        "target": target_path,
        "name": target_name,
        "status": "pending",
        "states": 0,
        "transitions": 0,
        "functions": 0,
        "invariants": 0,
        "vulnerabilities": 0,
        "severity_score": 0,
        "issues": []
    }

    try:
        # Create output directory
        output_dir = os.path.join(output_base, target_name)
        os.makedirs(output_dir, exist_ok=True)

        # Phase 1: X-Ray
        xray_result = extract_logic({
            "target_path": target_path,
            "output_dir": output_dir,
            "lpp_root": str(LPP_ROOT)
        })

        if xray_result.get("error"):
            result["status"] = "xray_failed"
            result["error"] = xray_result["error"]
            return result

        bone_json = xray_result["bone_json"]
        meta = bone_json.get("_lvp_meta", {})

        result["states"] = len(bone_json.get("states", {}))
        result["transitions"] = len(bone_json.get("transitions", []))
        result["functions"] = len(meta.get("functions", []))
        result["source_lines"] = meta.get("source_lines", 0)

        # Phase 2: Threat Model (heuristic only for speed)
        invariants, threat_model = _generate_heuristic_invariants(bone_json, meta)
        result["invariants"] = len(invariants)

        # Phase 3: Heuristic vulnerability detection
        issues = []

        # Check for ungated transitions to sensitive states
        sensitive_keywords = ["execute", "delete", "admin", "write", "remove", "drop", "update"]
        transitions = bone_json.get("transitions", [])
        gates = bone_json.get("gates", {})

        for trans in transitions:
            to_state = trans.get("to", "").lower()
            has_gates = bool(trans.get("gates"))

            if any(kw in to_state for kw in sensitive_keywords) and not has_gates:
                issues.append({
                    "type": "UNGATED_SENSITIVE",
                    "severity": "high",
                    "description": f"Ungated transition to sensitive state '{trans.get('to')}'",
                    "transition": trans.get("id")
                })

        # Check for wildcard transitions
        for trans in transitions:
            if trans.get("from") == "*" and not trans.get("gates"):
                issues.append({
                    "type": "UNGATED_WILDCARD",
                    "severity": "medium",
                    "description": f"Ungated wildcard transition to '{trans.get('to')}'",
                    "transition": trans.get("id")
                })

        # Check for missing error handling
        states = bone_json.get("states", {})
        if "error" not in states and result["states"] > 3:
            issues.append({
                "type": "NO_ERROR_STATE",
                "severity": "medium",
                "description": "No error state defined in state machine"
            })

        # Check for potential infinite loops (states with self-transitions only)
        state_outgoing = {s: [] for s in states}
        for trans in transitions:
            from_state = trans.get("from")
            if from_state in state_outgoing:
                state_outgoing[from_state].append(trans.get("to"))

        for state, targets in state_outgoing.items():
            if targets and all(t == state for t in targets):
                issues.append({
                    "type": "POTENTIAL_INFINITE_LOOP",
                    "severity": "low",
                    "description": f"State '{state}' has only self-transitions"
                })

        result["issues"] = issues
        result["vulnerabilities"] = len([i for i in issues if i["severity"] in ("high", "critical")])

        # Calculate severity score
        severity_weights = {"critical": 10, "high": 7, "medium": 4, "low": 1}
        total = sum(severity_weights.get(i["severity"], 0) for i in issues)
        result["severity_score"] = min(10, total / max(1, len(issues))) if issues else 0

        # Generate unit tests if issues found
        if issues:
            # Convert issues to counter_examples format for test generation
            counter_examples = []
            for i, issue in enumerate(issues):
                ce = {
                    "id": f"CE_{i+1:03d}",
                    "violated_invariant": issue.get("type"),
                    "invariant_name": issue.get("description"),
                    "severity": issue.get("severity"),
                    "attack_vector": issue.get("description"),
                    "trace": [
                        {"state_num": 0, "state": "idle"},
                        {"state_num": 1, "state": issue.get("transition", "vulnerable"),
                         "event": issue.get("type", "ATTACK")}
                    ]
                }
                counter_examples.append(ce)

            # Generate mock-based tests (always works)
            test_result = generate_unittests({
                "counter_examples": counter_examples,
                "bone_json": bone_json,
                "target_path": target_path,
                "target_name": target_name,
                "output_dir": output_dir
            })

            result["test_file"] = test_result.get("test_file")
            result["test_count"] = test_result.get("test_count", 0)

            # Generate direct tests (calls actual code)
            # Uses LLM if API key available, otherwise static analysis
            direct_test_result = generate_llm_unittests({
                "counter_examples": counter_examples,
                "bone_json": bone_json,
                "target_path": target_path,
                "target_name": target_name,
                "output_dir": output_dir
            })

            result["direct_test_file"] = direct_test_result.get("test_file")
            result["direct_test_count"] = direct_test_result.get("test_count", 0)

        # Analyze coverage based on logic backbone
        coverage = analyze_coverage({
            "bone_json": bone_json,
            "counter_examples": counter_examples if issues else [],
            "output_dir": output_dir
        })

        result["coverage"] = coverage.get("summary", {})

        # Generate tests to fill coverage gaps
        if coverage.get("gaps", {}).get("untested_states"):
            cov_tests = generate_coverage_tests({
                "bone_json": bone_json,
                "coverage_report": coverage,
                "target_name": target_name,
                "output_dir": output_dir
            })
            result["coverage_test_file"] = cov_tests.get("test_file")
            result["coverage_test_count"] = cov_tests.get("test_count", 0)

        result["status"] = "complete"

        # Save individual report
        report_path = os.path.join(output_dir, "audit_result.json")
        with open(report_path, "w") as f:
            json.dump(result, f, indent=2)

    except Exception as e:
        result["status"] = "error"
        result["error"] = str(e)

    return result


def main():
    """Run batch audit on all L++ components."""
    print("=" * 70)
    print("  L++ LOGIC VULNERABILITY POINTER - BATCH AUDIT")
    print("=" * 70)

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_base = os.path.join(str(LPP_ROOT), "workflows", "logic_vulnerability_pointer", "results", f"batch_{run_id}")
    os.makedirs(output_base, exist_ok=True)

    print(f"\n[*] Run ID: {run_id}")
    print(f"[*] Output: {output_base}")

    # Collect all targets
    targets = []

    # Utils
    utils_dir = LPP_ROOT / "utils"
    for compute_file in utils_dir.glob("*/src/*_compute.py"):
        targets.append(str(compute_file))

    # Also check for cli files
    for cli_file in utils_dir.glob("*/src/*_cli.py"):
        targets.append(str(cli_file))

    # Workflows
    workflows_dir = LPP_ROOT / "workflows"
    for compute_file in workflows_dir.glob("*/src/*_compute.py"):
        targets.append(str(compute_file))

    print(f"[*] Found {len(targets)} targets to audit\n")

    results = []
    total_vulns = 0
    total_issues = 0

    for i, target in enumerate(targets, 1):
        name = Path(target).stem
        parent = Path(target).parent.parent.name
        print(f"[{i:2}/{len(targets)}] Auditing {parent}/{name}...", end=" ", flush=True)

        result = audit_file(target, output_base)
        if result:
            results.append(result)

            vulns = result.get("vulnerabilities", 0)
            issues = len(result.get("issues", []))
            total_vulns += vulns
            total_issues += issues

            if result["status"] == "complete":
                if vulns > 0:
                    print(f"ISSUES FOUND ({vulns} high+, {issues} total)")
                else:
                    print(f"OK ({result['states']} states, {result['functions']} functions)")
            else:
                print(f"FAILED: {result.get('error', 'Unknown')[:50]}")

    # Generate summary report
    print("\n" + "=" * 70)
    print("  BATCH AUDIT SUMMARY")
    print("=" * 70)

    summary = {
        "run_id": run_id,
        "timestamp": datetime.now().isoformat(),
        "total_targets": len(results),
        "total_issues": total_issues,
        "total_high_vulns": total_vulns,
        "by_severity": {
            "critical": sum(1 for r in results for i in r.get("issues", []) if i["severity"] == "critical"),
            "high": sum(1 for r in results for i in r.get("issues", []) if i["severity"] == "high"),
            "medium": sum(1 for r in results for i in r.get("issues", []) if i["severity"] == "medium"),
            "low": sum(1 for r in results for i in r.get("issues", []) if i["severity"] == "low")
        },
        "totals": {
            "states": sum(r.get("states", 0) for r in results),
            "transitions": sum(r.get("transitions", 0) for r in results),
            "functions": sum(r.get("functions", 0) for r in results),
            "source_lines": sum(r.get("source_lines", 0) for r in results)
        },
        "results": results
    }

    print(f"\n  Targets Audited: {len(results)}")
    print(f"  Total States:    {summary['totals']['states']}")
    print(f"  Total Functions: {summary['totals']['functions']}")
    print(f"  Source Lines:    {summary['totals']['source_lines']}")
    print(f"\n  Issues Found:")
    print(f"    Critical: {summary['by_severity']['critical']}")
    print(f"    High:     {summary['by_severity']['high']}")
    print(f"    Medium:   {summary['by_severity']['medium']}")
    print(f"    Low:      {summary['by_severity']['low']}")

    # List files with issues
    files_with_issues = [r for r in results if r.get("issues")]
    if files_with_issues:
        print(f"\n  Files with Issues ({len(files_with_issues)}):")
        for r in sorted(files_with_issues, key=lambda x: -x.get("vulnerabilities", 0)):
            print(f"    - {r['name']}: {len(r['issues'])} issues ({r.get('vulnerabilities', 0)} high+)")
            for issue in r.get("issues", [])[:3]:  # Show first 3
                print(f"        [{issue['severity'].upper():8}] {issue['description'][:50]}")

    # Save summary
    summary_path = os.path.join(output_base, "batch_summary.json")
    with open(summary_path, "w") as f:
        json.dump(summary, f, indent=2)

    print(f"\n  Full report: {summary_path}")
    print("=" * 70)

    return summary


if __name__ == "__main__":
    main()
