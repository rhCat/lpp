# L++ Logic Vulnerability Pointer (LVP)

## Non-Destructive Testing (NDT) for Software Logic

In the physical world, we use X-rays or ultrasound to find cracks in a turbine blade before it shatters. LVP does the same for software—it finds **"Logic Cracks"** in Python applications and generates **"Adversarial Stress"** (test scripts) to prove they exist.

```
╔═══════════════════════════════════════════════════════════════╗
║   L++ LOGIC VULNERABILITY POINTER (LVP)                       ║
║   Non-Destructive Testing for Software Logic                  ║
╠═══════════════════════════════════════════════════════════════╣
║   Phase 1: X-Ray       - Extract logic blueprint              ║
║   Phase 2: Threat Model - Define safety invariants            ║
║   Phase 3: Stress Test - Find counter-examples (TLC)          ║
║   Phase 4: Exploit Gen - Generate trigger scripts             ║
║   Phase 5: The Fix     - Re-machine with TLAPS immunity       ║
╚═══════════════════════════════════════════════════════════════╝
```

## The LVP Workflow: "Search and Destroy"

| Phase | Name | Tool | Description |
|-------|------|------|-------------|
| 1 | **X-Ray** | `logic_decoder` | Extract the "Bone" JSON (logic blueprint) from target Python code |
| 2 | **Threat Model** | `llm_assistant` | Define Safety Invariants using LLM analysis |
| 3 | **Stress Test** | `tla_validator` | Run TLC model checker to find Counter-example Traces |
| 4 | **Exploit Gen** | `lvp` | Convert TLA+ traces into Python Trigger Scripts |
| 5 | **The Fix** | `skill_contractor` | Re-machine the JSON to add missing GATEs and generate TLAPS Seal |

## Quick Start

### Basic Usage

```bash
# Full security audit
python interactive.py target_app.py

# Only extract logic blueprint (X-Ray)
python interactive.py target_app.py --xray

# Stop after threat modeling
python interactive.py target_app.py --threat

# Skip auto-fix generation
python interactive.py target_app.py --no-fix
```

### Example: Auditing a Payment System

```bash
# Audit a hypothetical payment processing module
python interactive.py ~/projects/payment_gateway/processor.py

# Output:
# [+] Run ID: 20250105_143022
# [+] Target: processor
#
# ═══════════════════════════════════════════════════════════════
#   PHASE 1: X-RAY
#   Extracting logic blueprint (the 'Bone')
# ═══════════════════════════════════════════════════════════════
#
# [+] Logic blueprint extracted:
#     - States: 8
#     - Transitions: 15
#     - Functions: 12
#
# ═══════════════════════════════════════════════════════════════
#   PHASE 2: THREAT MODEL
#   Defining safety invariants
# ═══════════════════════════════════════════════════════════════
#
# [+] Defined 7 safety invariants:
#     - [CRITICAL ] Authorization Gate for refund
#     - [HIGH     ] Payment state consistency
#     ...
#
# [!] VULNERABILITIES FOUND: 2
# [+] Severity Score: 8.5/10
#
#     [CRITICAL ] CE_001: Ungated Refund Access
#         Attack: Transition to execute_refund has no authorization gates
```

## Output Artifacts

After running LVP, the `results/<run_id>/` directory contains:

```
results/20250105_143022/
├── bone.json              # Extracted logic blueprint
├── threat_model.json      # Invariants and threat analysis
├── security_spec.tla      # TLA+ specification with invariants
├── security_spec.cfg      # TLC configuration
├── tlc_output.txt         # Raw TLC output
├── patched_bone.json      # Fixed blueprint with added gates
├── patches.json           # List of patches applied
├── tlaps_proof.json       # Formal verification proof
├── lvp_report.json        # Full JSON report
├── lvp_report.md          # Human-readable report
└── exploits/
    ├── ce_001_poc.py      # PoC for vulnerability 1
    ├── ce_002_poc.py      # PoC for vulnerability 2
    └── run_all_exploits.py  # Master exploit runner
```

## How It Works

### Phase 1: X-Ray (Logic Extraction)

LVP uses the `logic_decoder` to perform AST analysis on the target Python file:

1. Parse Python source into AST
2. Analyze imports to identify semantic categories (HTTP, database, auth, etc.)
3. Extract functions, classes, and control flow patterns
4. Infer state machine structure
5. Generate the "Bone" JSON blueprint

**Example Bone JSON:**
```json
{
  "$schema": "lpp/v0.1",
  "id": "decoded_processor",
  "states": {
    "idle": { "description": "Awaiting request" },
    "processing": { "description": "Processing payment" },
    "authorizing": { "description": "Checking authorization" },
    "executing": { "description": "Executing transaction" },
    "complete": { "description": "Transaction complete" }
  },
  "transitions": [
    { "id": "t1", "from": "idle", "to": "processing", "on_event": "SUBMIT" },
    { "id": "t2", "from": "processing", "to": "authorizing", "on_event": "VALIDATE_OK" },
    { "id": "t3", "from": "authorizing", "to": "executing", "on_event": "AUTH_OK", "gates": ["is_authorized"] }
  ],
  "gates": {
    "is_authorized": { "type": "expression", "expression": "auth_level >= required_level" }
  }
}
```

### Phase 2: Threat Model (Invariant Definition)

LVP analyzes the blueprint to define safety invariants—properties that must **ALWAYS** hold true:

1. **Authorization Bypasses**: Can privileged actions be reached without checks?
2. **State Corruption**: Can the system reach invalid state combinations?
3. **Resource Leaks**: Can resources be allocated without being freed?
4. **Deadlock**: Can the system get stuck in non-terminal states?

**Example Invariants:**
```json
{
  "invariants": [
    {
      "id": "INV_001",
      "name": "Refund Authorization",
      "description": "execute_refund requires manager_approval",
      "tla_expression": "state = \"execute_refund\" => manager_approval = TRUE",
      "severity": "critical",
      "attack_vector": "Bypassing manager approval to execute unauthorized refunds"
    }
  ]
}
```

### Phase 3: Stress Test (TLC Model Checking)

LVP generates a TLA+ specification and runs the TLC model checker:

1. Convert blueprint to TLA+ with custom invariants
2. Run TLC to exhaustively explore the state space
3. If TLC finds a path that violates an invariant, it returns a **counter-example trace**

**Counter-Example Trace:**
```
State 0: idle
State 1: processing (via SUBMIT)
State 2: execute_refund (via FORCE_REFUND)  <-- Invariant violated!
```

### Phase 4: Exploit Gen (Trigger Scripts)

The **key innovation** of LVP: Converting abstract TLA+ traces into executable Python scripts.

For each counter-example, LVP generates:

```python
class ExploitPoC:
    """
    Proof of Concept for: Refund Authorization Bypass
    Attack Vector: Bypassing manager approval to execute unauthorized refunds
    """

    def execute(self):
        """
        Trace:
            State 0: idle
            State 1: processing (via SUBMIT)
            State 2: execute_refund (via FORCE_REFUND)
        """
        # Step 1: Start in idle state
        self.steps_executed.append('idle')

        # Step 2: Trigger 'SUBMIT' -> 'processing'
        print("[*] Step 2: SUBMIT -> processing")
        # TODO: Call target.submit_payment()

        # Step 3: Trigger 'FORCE_REFUND' -> 'execute_refund'
        print("[*] Step 3: FORCE_REFUND -> execute_refund")
        # TODO: Call target.force_refund() - BYPASSES AUTHORIZATION!

        return self.verify()
```

### Phase 5: The Fix (Gate Insertion + TLAPS Proof)

LVP generates patches to add missing security gates:

1. Identify the vulnerable transition
2. Generate an authorization gate expression
3. Insert the gate into the transition
4. Re-run TLC to verify the fix
5. Generate TLAPS proof of immunity

**Patch Example:**
```json
{
  "id": "PATCH_001",
  "fixes": "CE_001",
  "vulnerability": "Refund Authorization Bypass",
  "gate_added": "fix_gate_ce_001",
  "gate_expression": "manager_approval == True",
  "transition_modified": "t_force_refund",
  "description": "Add authorization gate to force_refund transition"
}
```

## Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `LPP_ROOT` | L++ framework root directory | Auto-detected |
| `OPENAI_API_KEY` | API key for LLM-assisted threat modeling | None |
| `OPENAI_API_BASE` | LLM API base URL | `https://api.openai.com/v1` |
| `LLM_MODEL` | LLM model to use | `gpt-4` |

### TLC Configuration

TLC model checking can be configured via the generated `.cfg` file:

```
# State space bounds
CONSTRAINT StateConstraint

# Check these invariants
INVARIANT TypeInvariant
INVARIANT AlwaysValidState
INVARIANT INV_001
INVARIANT INV_002

# Deadlock checking (disabled by default for L++ state machines)
CHECK_DEADLOCK FALSE
```

## Integration with CI/CD

Add LVP to your CI pipeline to catch logic vulnerabilities before deployment:

```yaml
# .github/workflows/security.yml
name: Security Audit

on: [push, pull_request]

jobs:
  lvp-audit:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Run LVP Audit
        run: |
          cd $LPP_ROOT/workflows/logic_vulnerability_pointer
          python interactive.py src/main.py --no-fix

      - name: Upload Report
        uses: actions/upload-artifact@v3
        with:
          name: lvp-report
          path: results/*/lvp_report.md
```

## Advanced Usage

### Custom Invariants

You can define custom invariants in a JSON file:

```json
{
  "custom_invariants": [
    {
      "id": "CUSTOM_001",
      "name": "Rate Limiting",
      "tla_expression": "request_count <= MAX_REQUESTS",
      "severity": "high"
    }
  ]
}
```

Then pass to LVP:

```bash
python interactive.py target.py --invariants custom_invariants.json
```

### Batch Auditing

Audit multiple files:

```bash
for f in src/*.py; do
  python interactive.py "$f" --xray
done
```

### Integration with skill_contractor

After finding vulnerabilities, use `skill_contractor` to automatically generate the fixed implementation:

```bash
# 1. Run LVP to find vulnerabilities
python interactive.py target.py

# 2. Use skill_contractor to implement the fix
cd ../../../utils/skill_contractor
python interactive.py "Implement security fix from patches.json"
```

## Architecture

```
logic_vulnerability_pointer/
├── logic_vulnerability_pointer.json  # Workflow blueprint (state machine)
├── interactive.py                    # CLI interface
├── README.md                         # This documentation
├── src/
│   ├── __init__.py
│   └── lvp_compute.py               # Compute functions
├── results/                         # Audit outputs
│   └── <run_id>/
│       ├── bone.json
│       ├── exploits/
│       └── lvp_report.md
└── tla/                             # TLA+ specifications
```

## State Machine

The LVP workflow follows this state machine:

```
idle
  │
  ├─[AUDIT]→ xray ─[DONE]→ threat_modeling ─[DONE]→ stress_testing
  │                                                     │
  │                    ┌──────────────────────────────┘
  │                    │
  │                    ├─[no vulnerabilities]→ secure
  │                    │
  │                    └─[vulnerabilities found]→ analyzing_traces
  │                                                    │
  │                                                    ↓
  │                                           exploit_generation
  │                                                    │
  │                                    ┌───────────────┴───────────────┐
  │                                    │                               │
  │                            [auto_fix=true]                  [auto_fix=false]
  │                                    │                               │
  │                                    ↓                               │
  │                            generating_fix                          │
  │                                    │                               │
  │                                    ↓                               │
  │                            verifying_fix                           │
  │                                    │                               │
  │                                    └───────────────┬───────────────┘
  │                                                    │
  │                                                    ↓
  │                                               reporting
  │                                                    │
  │                                                    ↓
  └─────────────────────────────────────────────→ complete
```

## Security Considerations

- LVP generates **proof-of-concept exploit scripts** for authorized security testing only
- Always run LVP in a controlled environment
- Generated exploits are for defensive purposes—to demonstrate vulnerabilities to stakeholders
- Never use LVP against systems without explicit authorization

## Contributing

1. Fork the repository
2. Create a feature branch
3. Submit a pull request with tests

## License

Part of the L++ Framework. See root LICENSE file.
