"""
L++ Logic Vulnerability Pointer (LVP) - Compute Units

Non-Destructive Testing (NDT) for software logic.
Like X-rays for turbine blades, LVP finds "logic cracks" in Python applications.

Phases:
1. X-Ray: Extract logic blueprint ("Bone" JSON)
2. Threat Model: Define safety invariants with LLM
3. Stress Test: Run TLC model checker to find counter-examples
4. Exploit Gen: Convert TLA+ traces to Python trigger scripts
5. The Fix: Re-machine logic with missing gates + TLAPS proof
"""

import ast
import json
import os
import re
import subprocess
import sys
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

# Try to import LLM client (OpenAI-compatible)
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None


# ============================================================================
# PHASE 0: INITIALIZATION
# ============================================================================

def init(params: dict) -> dict:
    """Initialize the LVP audit environment."""
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Find L++ root
    lpp_root = os.environ.get("LPP_ROOT", "")
    if not lpp_root:
        # Try to find it relative to this file
        current = Path(__file__).resolve()
        for parent in current.parents:
            if (parent / "src" / "frame_py").exists():
                lpp_root = str(parent)
                break

    # Setup output directory
    output_dir = os.path.join(lpp_root, "workflows", "logic_vulnerability_pointer", "results", run_id)
    os.makedirs(output_dir, exist_ok=True)

    return {
        "run_id": run_id,
        "output_dir": output_dir,
        "phase": "init",
        "lpp_root": lpp_root,
        "api_key": os.environ.get("OPENAI_API_KEY", os.environ.get("LLM_API_KEY", "")),
        "api_base": os.environ.get("OPENAI_API_BASE", os.environ.get("LLM_API_BASE", "https://api.openai.com/v1")),
        "model": os.environ.get("LLM_MODEL", "gpt-4"),
        "auto_fix": True
    }


def set_target_name(params: dict) -> dict:
    """Extract target name from path."""
    target_path = params.get("target_path", "")
    if target_path:
        name = Path(target_path).stem
        # Clean up name
        name = re.sub(r'[^a-zA-Z0-9_]', '_', name)
        return {"target_name": name}
    return {"target_name": "unknown_target"}


# ============================================================================
# PHASE 1: THE X-RAY - Logic Blueprint Extraction
# ============================================================================

def extract_logic(params: dict) -> dict:
    """
    Phase 1: X-Ray - Extract the "Bone" JSON (logic blueprint) from target.

    Uses logic_decoder functions to analyze Python code and extract:
    - State machine structure
    - Control flow patterns
    - Gates (conditions)
    - Actions (side effects)
    """
    target_path = params.get("target_path", "")
    output_dir = params.get("output_dir", "")
    lpp_root = params.get("lpp_root", "")

    if not target_path or not os.path.exists(target_path):
        return {"bone_json": None, "bone_path": None, "error": f"Target not found: {target_path}"}

    # Add L++ to path for imports
    if lpp_root:
        sys.path.insert(0, lpp_root)

    try:
        # Import logic decoder functions
        from utils.logic_decoder.src.decoder_compute import (
            loadFile, parseAst, analyzeImports, analyzeFunctions,
            extractConstants, analyzeControlFlow, inferStates,
            inferTransitions, inferActions, generateBlueprint
        )

        # Run the decoder pipeline
        # Step 1: Load file
        load_result = loadFile({"filePath": target_path})
        if load_result.get("error"):
            return {"bone_json": None, "bone_path": None, "error": load_result["error"]}

        source_code = load_result["sourceCode"]

        # Step 2: Parse AST
        ast_result = parseAst({"sourceCode": source_code})
        if ast_result.get("error"):
            return {"bone_json": None, "bone_path": None, "error": ast_result["error"]}

        ast_dict = ast_result["ast"]

        # Step 3: Analyze imports
        imports_result = analyzeImports({"ast": ast_dict})
        imports = imports_result.get("imports", [])

        # Step 4: Analyze functions
        functions_result = analyzeFunctions({"ast": ast_dict, "imports": imports})
        functions = functions_result.get("functions", [])
        classes = functions_result.get("classes", [])

        # Step 5: Extract constants
        constants_result = extractConstants({"ast": ast_dict})
        constants = constants_result.get("constants", [])

        # Step 6: Analyze control flow
        cfg_result = analyzeControlFlow({"ast": ast_dict, "functions": functions})
        control_flow = cfg_result.get("controlFlow", {})

        # Step 7: Infer states
        states_result = inferStates({
            "functions": functions,
            "classes": classes,
            "controlFlow": control_flow,
            "imports": imports
        })
        inferred_states = states_result.get("states", [])

        # Step 8: Infer transitions and gates
        trans_result = inferTransitions({
            "controlFlow": control_flow,
            "inferredStates": inferred_states,
            "functions": functions
        })
        transitions = trans_result.get("transitions", [])
        gates = trans_result.get("gates", [])

        # Step 9: Infer actions
        actions_result = inferActions({
            "functions": functions,
            "imports": imports,
            "controlFlow": control_flow
        })
        actions = actions_result.get("actions", [])

        # Step 10: Generate blueprint
        bp_result = generateBlueprint({
            "filePath": target_path,
            "inferredStates": inferred_states,
            "inferredTransitions": transitions,
            "inferredGates": gates,
            "inferredActions": actions,
            "imports": imports,
            "constants": constants,
            "classes": classes
        })

        bone_json = bp_result.get("blueprint", {})

        # Add source info for vulnerability analysis
        bone_json["_lvp_meta"] = {
            "source_path": target_path,
            "functions": [f["name"] for f in functions if not f["name"].startswith("_")],
            "classes": [c["name"] for c in classes],
            "imports": [(i.get("module") or i.get("name")) for i in imports],
            "control_patterns": control_flow.get("patterns", {}),
            "source_lines": len(source_code.split("\n"))
        }

        # Save the bone
        bone_path = os.path.join(output_dir, "bone.json")
        with open(bone_path, "w") as f:
            json.dump(bone_json, f, indent=2)

        return {"bone_json": bone_json, "bone_path": bone_path, "error": None}

    except Exception as e:
        return {"bone_json": None, "bone_path": None, "error": f"X-Ray extraction failed: {str(e)}"}


# ============================================================================
# PHASE 2: THREAT MODEL - Safety Invariant Definition
# ============================================================================

THREAT_MODEL_PROMPT = """You are a security auditor analyzing a software logic blueprint.

BLUEPRINT:
{blueprint}

SOURCE METADATA:
- Functions: {functions}
- Classes: {classes}
- Control Patterns: {patterns}

Your task is to identify SAFETY INVARIANTS - properties that must ALWAYS hold true.
Think about what could go wrong. Consider:

1. AUTHORIZATION BYPASSES: Can privileged actions be reached without proper checks?
   Example: "Agent must never reach execute_refund without manager_approval == True"

2. STATE CORRUPTION: Can the system reach invalid states?
   Example: "order_status cannot be 'shipped' while payment_status is 'pending'"

3. RESOURCE LEAKS: Can resources be allocated without being freed?
   Example: "connection_count must eventually return to 0"

4. INFINITE LOOPS: Can the system get stuck in non-terminal states?
   Example: "From any state, complete or error must eventually be reachable"

5. RACE CONDITIONS: Can concurrent operations cause conflicts?
   Example: "balance cannot go negative"

6. INPUT VALIDATION: Can malicious inputs bypass validation?
   Example: "user_input must be sanitized before execute_query"

Output your analysis as JSON:
{{
  "invariants": [
    {{
      "id": "INV_001",
      "name": "Human-readable name",
      "description": "What this invariant protects against",
      "tla_expression": "TLA+ expression (e.g., state = \\"privileged\\" => approved = TRUE)",
      "severity": "critical|high|medium|low",
      "attack_vector": "How an attacker might try to violate this"
    }}
  ],
  "threat_model": {{
    "assets": ["List of valuable assets/operations to protect"],
    "threats": ["List of potential threats identified"],
    "attack_surface": ["Entry points that could be exploited"]
  }}
}}

Generate at least 5 invariants, focusing on the most critical security properties first.
"""


def define_invariants(params: dict) -> dict:
    """
    Phase 2: Threat Model - Define safety invariants using LLM analysis.

    Analyzes the bone JSON to identify critical security properties that
    must always hold true (invariants).
    """
    bone_json = params.get("bone_json", {})
    target_name = params.get("target_name", "target")
    api_key = params.get("api_key", "")
    api_base = params.get("api_base", "https://api.openai.com/v1")
    model = params.get("model", "gpt-4")
    output_dir = params.get("output_dir", "")

    if not bone_json:
        return {"invariants": [], "threat_model": {}, "invariant_count": 0, "error": "No bone JSON to analyze"}

    # Extract metadata for context
    meta = bone_json.get("_lvp_meta", {})
    functions = meta.get("functions", [])
    classes = meta.get("classes", [])
    patterns = meta.get("control_patterns", {})

    # Clean blueprint for prompt (remove meta)
    clean_bp = {k: v for k, v in bone_json.items() if not k.startswith("_")}

    # Use LLM if available
    if api_key and OpenAI:
        try:
            client = OpenAI(api_key=api_key, base_url=api_base)

            prompt = THREAT_MODEL_PROMPT.format(
                blueprint=json.dumps(clean_bp, indent=2)[:4000],  # Truncate if too long
                functions=", ".join(functions[:20]),
                classes=", ".join(classes[:10]),
                patterns=json.dumps(patterns)
            )

            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "You are an expert security auditor specializing in formal verification. Output only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=2000
            )

            content = response.choices[0].message.content

            # Extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                result = json.loads(json_match.group())
                invariants = result.get("invariants", [])
                threat_model = result.get("threat_model", {})
            else:
                invariants = []
                threat_model = {}

        except Exception as e:
            # Fall back to heuristic invariants
            invariants, threat_model = _generate_heuristic_invariants(bone_json, meta)
    else:
        # No LLM - use heuristics
        invariants, threat_model = _generate_heuristic_invariants(bone_json, meta)

    # Save threat model
    if output_dir:
        threat_path = os.path.join(output_dir, "threat_model.json")
        with open(threat_path, "w") as f:
            json.dump({"invariants": invariants, "threat_model": threat_model}, f, indent=2)

    return {
        "invariants": invariants,
        "threat_model": threat_model,
        "invariant_count": len(invariants),
        "error": None
    }


def _generate_heuristic_invariants(bone_json: dict, meta: dict) -> Tuple[List[dict], dict]:
    """Generate invariants using heuristics when LLM is unavailable."""
    invariants = []
    inv_id = 1

    states = bone_json.get("states", {})
    transitions = bone_json.get("transitions", [])
    gates = bone_json.get("gates", {})
    terminal_states = bone_json.get("terminal_states", [])

    # Invariant 1: State validity
    invariants.append({
        "id": f"INV_{inv_id:03d}",
        "name": "Valid State",
        "description": "System must always be in a defined state",
        "tla_expression": "state \\in States",
        "severity": "critical",
        "attack_vector": "State corruption through invalid transitions"
    })
    inv_id += 1

    # Invariant 2: Error state reachability
    if "error" in states:
        invariants.append({
            "id": f"INV_{inv_id:03d}",
            "name": "Error Handling",
            "description": "Errors must be properly handled, not silently ignored",
            "tla_expression": "error /= NULL => state = \"error\"",
            "severity": "high",
            "attack_vector": "Bypassing error handlers to continue in invalid state"
        })
        inv_id += 1

    # Invariant 3: Terminal state properties
    for term in terminal_states:
        if term != "error":
            invariants.append({
                "id": f"INV_{inv_id:03d}",
                "name": f"Terminal State {term}",
                "description": f"Once in {term} state, no further transitions should occur",
                "tla_expression": f"state = \"{term}\" => state' = \"{term}\"",
                "severity": "medium",
                "attack_vector": f"Escaping terminal state {term} through unexpected event"
            })
            inv_id += 1

    # Invariant 4: Gate bypasses
    for trans in transitions:
        if trans.get("gates"):
            gate_ids = trans.get("gates", [])
            from_state = trans.get("from")
            to_state = trans.get("to")
            for gid in gate_ids:
                gate = gates.get(gid, {})
                expr = gate.get("expression", "")
                if "approval" in expr.lower() or "auth" in expr.lower() or "permission" in expr.lower():
                    invariants.append({
                        "id": f"INV_{inv_id:03d}",
                        "name": f"Authorization Gate {gid}",
                        "description": f"Transition from {from_state} to {to_state} requires authorization",
                        "tla_expression": f"state = \"{to_state}\" => ({gate.get('expression', 'TRUE')})",
                        "severity": "critical",
                        "attack_vector": f"Bypassing authorization check on transition to {to_state}"
                    })
                    inv_id += 1

    # Invariant 5: Deadlock freedom (if not terminal)
    invariants.append({
        "id": f"INV_{inv_id:03d}",
        "name": "Deadlock Freedom",
        "description": "System should eventually reach a terminal state or have available transitions",
        "tla_expression": "state \\in TerminalStates \\/ ENABLED Next",
        "severity": "medium",
        "attack_vector": "Causing system to hang in non-terminal state with no available transitions"
    })

    threat_model = {
        "assets": list(states.keys()),
        "threats": [
            "State transition bypass",
            "Gate condition circumvention",
            "Terminal state escape",
            "Error suppression"
        ],
        "attack_surface": [t.get("on_event") for t in transitions if t.get("on_event")]
    }

    return invariants, threat_model


# ============================================================================
# PHASE 3: STRESS TEST - TLC Model Checking
# ============================================================================

def generate_tla(params: dict) -> dict:
    """Generate TLA+ specification with custom invariants."""
    bone_json = params.get("bone_json", {})
    invariants = params.get("invariants", [])
    output_dir = params.get("output_dir", "")
    lpp_root = params.get("lpp_root", "")

    if not bone_json:
        return {"tla_spec": None, "tla_path": None, "error": "No bone JSON"}

    # Add L++ to path
    if lpp_root:
        sys.path.insert(0, lpp_root)

    try:
        from src.frame_py.tla_validator import generate_tla as gen_tla, generate_cfg

        # Generate base TLA+ spec
        tla_spec = gen_tla(bone_json, int_min=-5, int_max=5, max_history=10)

        # Inject custom invariants before the closing line
        custom_invariants = []
        for inv in invariants:
            inv_name = re.sub(r'[^a-zA-Z0-9_]', '_', inv.get("id", "CustomInv"))
            tla_expr = inv.get("tla_expression", "TRUE")
            comment = inv.get("description", "Custom invariant")
            custom_invariants.append(f"\\* {comment}")
            custom_invariants.append(f"{inv_name} == {tla_expr}")
            custom_invariants.append("")

        # Insert before the final line (====...)
        lines = tla_spec.split("\n")
        insert_idx = len(lines) - 1
        for i in range(len(lines) - 1, -1, -1):
            if lines[i].startswith("===="):
                insert_idx = i
                break

        # Insert custom invariants
        if custom_invariants:
            lines.insert(insert_idx, "\\* =========================================================")
            lines.insert(insert_idx + 1, "\\* SECURITY INVARIANTS - LVP Generated")
            lines.insert(insert_idx + 2, "\\* =========================================================")
            for i, inv_line in enumerate(custom_invariants):
                lines.insert(insert_idx + 3 + i, inv_line)

        tla_spec = "\n".join(lines)

        # Save TLA+ spec
        tla_path = os.path.join(output_dir, "security_spec.tla")
        with open(tla_path, "w") as f:
            f.write(tla_spec)

        # Generate and save config
        cfg = generate_cfg(bone_json, check_deadlock=False)

        # Add custom invariants to config
        cfg_lines = cfg.split("\n")
        for inv in invariants:
            inv_name = re.sub(r'[^a-zA-Z0-9_]', '_', inv.get("id", "CustomInv"))
            cfg_lines.append(f"INVARIANT {inv_name}")

        cfg = "\n".join(cfg_lines)
        cfg_path = os.path.join(output_dir, "security_spec.cfg")
        with open(cfg_path, "w") as f:
            f.write(cfg)

        return {"tla_spec": tla_spec, "tla_path": tla_path, "error": None}

    except Exception as e:
        return {"tla_spec": None, "tla_path": None, "error": f"TLA+ generation failed: {str(e)}"}


def run_tlc(params: dict) -> dict:
    """
    Phase 3: Stress Test - Run TLC model checker to find counter-examples.

    If TLC finds a way to violate an invariant, it returns a counter-example
    trace showing the exact sequence of events that breaks the safety property.
    """
    tla_path = params.get("tla_path", "")
    tla_spec = params.get("tla_spec", "")
    invariants = params.get("invariants", [])
    output_dir = params.get("output_dir", "")
    lpp_root = params.get("lpp_root", "")

    if not tla_path or not os.path.exists(tla_path):
        return {
            "tlc_result": {"status": "skipped", "reason": "No TLA+ spec"},
            "counter_examples": [],
            "vulnerability_count": 0,
            "error": None
        }

    # Try to find TLC
    tlc_paths = [
        "tlc",  # In PATH
        os.path.join(lpp_root, "tools", "tlc", "tlc"),
        "/usr/local/bin/tlc",
        os.path.expanduser("~/.local/bin/tlc")
    ]

    tlc_cmd = None
    for path in tlc_paths:
        try:
            result = subprocess.run([path, "-h"], capture_output=True, timeout=5)
            tlc_cmd = path
            break
        except (FileNotFoundError, subprocess.TimeoutExpired):
            continue

    if not tlc_cmd:
        # TLC not available - simulate with heuristic analysis
        return _heuristic_stress_test(params)

    # Run TLC
    cfg_path = tla_path.replace(".tla", ".cfg")
    try:
        result = subprocess.run(
            [tlc_cmd, "-config", cfg_path, tla_path, "-workers", "auto"],
            capture_output=True,
            text=True,
            timeout=120,
            cwd=output_dir
        )

        output = result.stdout + result.stderr

        # Parse TLC output for counter-examples
        counter_examples = _parse_tlc_output(output, invariants)

        # Save TLC output
        tlc_output_path = os.path.join(output_dir, "tlc_output.txt")
        with open(tlc_output_path, "w") as f:
            f.write(output)

        return {
            "tlc_result": {
                "status": "complete",
                "return_code": result.returncode,
                "output_path": tlc_output_path
            },
            "counter_examples": counter_examples,
            "vulnerability_count": len(counter_examples),
            "error": None
        }

    except subprocess.TimeoutExpired:
        return {
            "tlc_result": {"status": "timeout"},
            "counter_examples": [],
            "vulnerability_count": 0,
            "error": "TLC timed out after 120s"
        }
    except Exception as e:
        return {
            "tlc_result": {"status": "error"},
            "counter_examples": [],
            "vulnerability_count": 0,
            "error": f"TLC error: {str(e)}"
        }


def _parse_tlc_output(output: str, invariants: List[dict]) -> List[dict]:
    """Parse TLC output to extract counter-example traces."""
    counter_examples = []

    # Look for invariant violations
    violation_pattern = r"Error: Invariant (\w+) is violated"
    trace_pattern = r"State (\d+):\s*<([^>]+)>"

    violations = re.findall(violation_pattern, output)

    for violation in violations:
        inv_id = violation

        # Find the invariant details
        inv_details = next((inv for inv in invariants if inv.get("id", "").replace("-", "_") == inv_id), {})

        # Extract trace leading to violation
        trace_states = re.findall(trace_pattern, output)

        counter_examples.append({
            "id": f"CE_{len(counter_examples) + 1:03d}",
            "violated_invariant": inv_id,
            "invariant_name": inv_details.get("name", inv_id),
            "severity": inv_details.get("severity", "unknown"),
            "attack_vector": inv_details.get("attack_vector", "Unknown"),
            "trace": [
                {"state_num": int(s[0]), "state": s[1]}
                for s in trace_states
            ],
            "raw_trace": output[output.find("Error:"):output.find("Error:") + 2000] if "Error:" in output else ""
        })

    return counter_examples


def _heuristic_stress_test(params: dict) -> dict:
    """Fallback stress testing when TLC is not available."""
    bone_json = params.get("bone_json", {})
    invariants = params.get("invariants", [])

    counter_examples = []

    # Analyze for common vulnerability patterns
    states = bone_json.get("states", {})
    transitions = bone_json.get("transitions", [])
    gates = bone_json.get("gates", {})

    # Check 1: Ungated transitions to sensitive states
    sensitive_keywords = ["execute", "delete", "admin", "privileged", "transfer", "payment", "refund"]
    for trans in transitions:
        to_state = trans.get("to", "")
        has_gates = bool(trans.get("gates"))

        if any(kw in to_state.lower() for kw in sensitive_keywords) and not has_gates:
            counter_examples.append({
                "id": f"CE_{len(counter_examples) + 1:03d}",
                "violated_invariant": "HEUR_UNGATED_SENSITIVE",
                "invariant_name": "Ungated Sensitive Transition",
                "severity": "high",
                "attack_vector": f"Transition to {to_state} has no authorization gates",
                "trace": [
                    {"state_num": 0, "state": trans.get("from", "any")},
                    {"state_num": 1, "state": to_state, "event": trans.get("on_event")}
                ],
                "raw_trace": json.dumps(trans)
            })

    # Check 2: Wildcard transitions that bypass gates
    for trans in transitions:
        if trans.get("from") == "*":
            for other_trans in transitions:
                if other_trans.get("gates") and other_trans.get("to") == trans.get("to"):
                    counter_examples.append({
                        "id": f"CE_{len(counter_examples) + 1:03d}",
                        "violated_invariant": "HEUR_WILDCARD_BYPASS",
                        "invariant_name": "Wildcard Gate Bypass",
                        "severity": "critical",
                        "attack_vector": f"Wildcard transition bypasses gated access to {trans.get('to')}",
                        "trace": [
                            {"state_num": 0, "state": "any (wildcard)"},
                            {"state_num": 1, "state": trans.get("to"), "event": trans.get("on_event")}
                        ],
                        "raw_trace": json.dumps({"bypassing": other_trans, "via": trans})
                    })

    return {
        "tlc_result": {"status": "heuristic", "reason": "TLC not available"},
        "counter_examples": counter_examples,
        "vulnerability_count": len(counter_examples),
        "error": None
    }


def analyze_traces(params: dict) -> dict:
    """Analyze counter-example traces for exploitability."""
    counter_examples = params.get("counter_examples", [])
    bone_json = params.get("bone_json", {})
    threat_model = params.get("threat_model", {})
    api_key = params.get("api_key", "")
    api_base = params.get("api_base", "")
    model = params.get("model", "gpt-4")

    if not counter_examples:
        return {"counter_examples": [], "severity_score": 0, "error": None}

    # Calculate severity score
    severity_weights = {"critical": 10, "high": 7, "medium": 4, "low": 1}
    total_severity = sum(
        severity_weights.get(ce.get("severity", "medium"), 4)
        for ce in counter_examples
    )

    # Normalize to 0-10 scale
    severity_score = min(10, total_severity / max(1, len(counter_examples)))

    # Enrich traces with exploitation context
    for ce in counter_examples:
        ce["exploitability"] = "high" if ce.get("severity") in ("critical", "high") else "medium"
        ce["exploitation_steps"] = _derive_exploitation_steps(ce, bone_json)

    return {
        "counter_examples": counter_examples,
        "severity_score": round(severity_score, 1),
        "error": None
    }


def _derive_exploitation_steps(counter_example: dict, bone_json: dict) -> List[str]:
    """Derive step-by-step exploitation from counter-example trace."""
    steps = []
    trace = counter_example.get("trace", [])

    steps.append(f"1. Start in initial state")

    for i, state in enumerate(trace[1:], 2):
        event = state.get("event", "trigger event")
        target = state.get("state", "next state")
        steps.append(f"{i}. Trigger '{event}' to transition to '{target}'")

    steps.append(f"{len(trace) + 1}. Invariant '{counter_example.get('violated_invariant')}' is now violated")
    steps.append(f"{len(trace) + 2}. {counter_example.get('attack_vector', 'Exploitation complete')}")

    return steps


# ============================================================================
# PHASE 4: EXPLOIT GEN - Python Trigger Script Generation
# ============================================================================

EXPLOIT_TEMPLATE = '''#!/usr/bin/env python3
"""
LVP Exploit PoC: {exploit_id}
Target: {target_name}
Vulnerability: {vulnerability}
Severity: {severity}

This script demonstrates the logic vulnerability by triggering
the exact sequence of events that violates the safety invariant.

DISCLAIMER: This is a Proof-of-Concept for authorized security testing only.
"""

import sys
import json
from pathlib import Path

# Add target to path
sys.path.insert(0, str(Path(__file__).parent.parent))

{imports}

class ExploitPoC:
    """
    Proof of Concept for: {vulnerability}

    Attack Vector: {attack_vector}
    """

    def __init__(self):
        self.name = "{exploit_id}"
        self.target = "{target_name}"
        self.steps_executed = []

    def setup(self):
        """Initialize the target in a known state."""
        print(f"[*] Setting up exploit: {{self.name}}")
        {setup_code}

    def execute(self):
        """
        Execute the attack sequence.

        Trace:
{trace_comment}
        """
        print(f"[*] Executing attack sequence...")

{attack_code}

        print(f"[+] Attack sequence complete")
        return self.verify()

    def verify(self):
        """Verify the invariant was violated."""
        print(f"[*] Verifying vulnerability...")
        {verify_code}

    def cleanup(self):
        """Clean up after exploit."""
        print(f"[*] Cleaning up...")
        {cleanup_code}

    def run(self):
        """Run the complete PoC."""
        print("=" * 60)
        print(f"LVP Exploit PoC: {{self.name}}")
        print(f"Target: {{self.target}}")
        print("=" * 60)

        try:
            self.setup()
            result = self.execute()
            if result:
                print(f"[+] VULNERABLE: Invariant violation confirmed")
            else:
                print(f"[-] NOT VULNERABLE: Invariant held")
            return result
        except Exception as e:
            print(f"[!] Exploit failed: {{e}}")
            return False
        finally:
            self.cleanup()


if __name__ == "__main__":
    poc = ExploitPoC()
    success = poc.run()
    sys.exit(0 if success else 1)
'''


def generate_exploits(params: dict) -> dict:
    """
    Phase 4: Exploit Gen - Convert counter-example traces to Python trigger scripts.

    This is the core LVP capability: transforming abstract TLA+ traces into
    concrete, executable Python scripts that demonstrate the vulnerability.
    """
    counter_examples = params.get("counter_examples", [])
    bone_json = params.get("bone_json", {})
    target_path = params.get("target_path", "")
    target_name = params.get("target_name", "target")
    output_dir = params.get("output_dir", "")
    api_key = params.get("api_key", "")
    api_base = params.get("api_base", "")
    model = params.get("model", "gpt-4")

    if not counter_examples:
        return {
            "exploits": [],
            "exploit_paths": [],
            "poc_generated": False,
            "error": None
        }

    exploits = []
    exploit_paths = []

    # Create exploits directory
    exploits_dir = os.path.join(output_dir, "exploits")
    os.makedirs(exploits_dir, exist_ok=True)

    for ce in counter_examples:
        exploit_id = ce.get("id", f"exploit_{len(exploits)}")

        # Generate trace comment
        trace_comment = ""
        for step in ce.get("trace", []):
            trace_comment += f"        #   State {step.get('state_num', '?')}: {step.get('state', 'unknown')}"
            if step.get("event"):
                trace_comment += f" (via {step['event']})"
            trace_comment += "\n"

        # Generate attack code from trace
        attack_code = _generate_attack_code(ce, bone_json, target_path)

        # Generate verification code
        verify_code = _generate_verify_code(ce, bone_json)

        # Build exploit script
        exploit_code = EXPLOIT_TEMPLATE.format(
            exploit_id=exploit_id,
            target_name=target_name,
            vulnerability=ce.get("invariant_name", "Unknown"),
            severity=ce.get("severity", "unknown"),
            attack_vector=ce.get("attack_vector", "Unknown"),
            imports=_generate_imports(target_path),
            setup_code="pass  # Target-specific setup",
            trace_comment=trace_comment,
            attack_code=attack_code,
            verify_code=verify_code,
            cleanup_code="pass  # Target-specific cleanup"
        )

        # Save exploit
        exploit_path = os.path.join(exploits_dir, f"{exploit_id.lower()}_poc.py")
        with open(exploit_path, "w") as f:
            f.write(exploit_code)

        exploits.append({
            "id": exploit_id,
            "vulnerability": ce.get("invariant_name"),
            "severity": ce.get("severity"),
            "path": exploit_path,
            "trace": ce.get("trace", [])
        })
        exploit_paths.append(exploit_path)

    # Generate master exploit runner
    runner_path = _generate_exploit_runner(exploits, exploits_dir, target_name)
    exploit_paths.append(runner_path)

    return {
        "exploits": exploits,
        "exploit_paths": exploit_paths,
        "poc_generated": True,
        "error": None
    }


def _generate_attack_code(counter_example: dict, bone_json: dict, target_path: str) -> str:
    """Generate the attack sequence code from the trace."""
    lines = []
    trace = counter_example.get("trace", [])

    meta = bone_json.get("_lvp_meta", {})
    functions = meta.get("functions", [])

    for i, step in enumerate(trace):
        event = step.get("event", "")
        target_state = step.get("state", "")

        lines.append(f"        # Step {i + 1}: Trigger '{event}' -> '{target_state}'")
        lines.append(f"        self.steps_executed.append('{target_state}')")

        # Try to map event to a function call
        if event:
            # Look for matching function
            matching_fn = None
            event_lower = event.lower().replace("_", "")
            for fn in functions:
                fn_lower = fn.lower().replace("_", "")
                if event_lower in fn_lower or fn_lower in event_lower:
                    matching_fn = fn
                    break

            if matching_fn:
                lines.append(f"        # TODO: Call target.{matching_fn}() with appropriate args")
                lines.append(f"        print(f\"[*] Step {i + 1}: {event} -> {target_state}\")")
            else:
                lines.append(f"        # TODO: Trigger event '{event}'")
                lines.append(f"        print(f\"[*] Step {i + 1}: {event} -> {target_state}\")")
        else:
            lines.append(f"        print(f\"[*] Step {i + 1}: -> {target_state}\")")

        lines.append("")

    if not lines:
        lines.append("        # No specific attack steps - manual exploitation required")
        lines.append("        pass")

    return "\n".join(lines)


def _generate_verify_code(counter_example: dict, bone_json: dict) -> str:
    """Generate code to verify the invariant was violated."""
    invariant = counter_example.get("violated_invariant", "")

    lines = [
        f"# Verify invariant '{invariant}' was violated",
        "# TODO: Add target-specific verification",
        "violated = len(self.steps_executed) == len(self.steps_executed)  # Placeholder",
        "return violated"
    ]

    return "\n        ".join(lines)


def _generate_imports(target_path: str) -> str:
    """Generate import statements for the target."""
    if not target_path:
        return "# No target imports"

    target_name = Path(target_path).stem
    return f"# from {target_name} import *  # Uncomment to import target"


# ============================================================================
# UNITTEST GENERATION - Pytest-compatible test cases
# ============================================================================

UNITTEST_TEMPLATE = '''#!/usr/bin/env python3
"""
LVP Security Test Suite: {target_name}
Generated: {timestamp}

These tests verify that logic vulnerabilities have been fixed.
Run with: pytest {test_file} -v

Each test case represents a counter-example trace that violated
a safety invariant. If the vulnerability is fixed, the test should FAIL
(meaning the attack sequence no longer works).
"""

import pytest
import sys
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from dataclasses import dataclass
from typing import Any, Dict, List, Optional

# Add target to path
sys.path.insert(0, str(Path(__file__).parent.parent))

{imports}


# ============================================================================
# Test Fixtures
# ============================================================================

@pytest.fixture
def initial_state():
    """Fixture providing clean initial state for each test."""
    return {{
        "state": "idle",
        "context": {{}},
        "history": []
    }}


@pytest.fixture
def state_machine():
    """Fixture providing a mock state machine for testing."""
    class MockStateMachine:
        def __init__(self):
            self.state = "idle"
            self.context = {{}}
            self.history = []
            self.transitions_taken = []

        def dispatch(self, event: str, payload: dict = None):
            """Dispatch an event to the state machine."""
            self.history.append({{"event": event, "from": self.state}})
            # Record transition
            self.transitions_taken.append(event)
            return True

        def get_state(self) -> str:
            return self.state

        def set_state(self, state: str):
            self.state = state

        def can_transition(self, event: str) -> bool:
            """Check if transition is allowed (for gate testing)."""
            return True

    return MockStateMachine()


# ============================================================================
# Vulnerability Test Cases
# ============================================================================

{test_cases}


# ============================================================================
# Test Runner
# ============================================================================

class TestSecurityRegression:
    """
    Security regression tests.

    These tests should PASS if vulnerabilities exist (demonstrating the flaw).
    After fixing, these tests should FAIL (attack no longer works).

    Use --invert flag or mark as xfail after fixes are applied.
    """

{regression_tests}


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
'''

SINGLE_TEST_TEMPLATE = '''
class Test{test_class_name}:
    """
    Test for: {vulnerability_name}
    Severity: {severity}

    Attack Vector: {attack_vector}

    This test attempts to trigger the vulnerability by following
    the counter-example trace that violated the safety invariant.
    """

    def test_vulnerability_exists(self, state_machine):
        """
        Test that demonstrates the vulnerability.

        Trace:
{trace_comment}

        Expected: If vulnerable, attack sequence completes successfully.
        After fix: This test should FAIL (gates block the attack).
        """
{test_steps}

        # Verify the attack reached the vulnerable state
        assert len(state_machine.transitions_taken) >= {min_transitions}, \\
            f"Attack sequence incomplete: {{state_machine.transitions_taken}}"

    def test_invariant_holds_with_gate(self, state_machine):
        """
        Test that the safety invariant holds when proper gates are in place.

        Invariant: {invariant_expression}

        This test verifies that unauthorized access is blocked.
        """
        # Setup: Ensure required authorization is NOT present
        state_machine.context["authorized"] = False
        state_machine.context["approved"] = False

{gate_test_steps}

        # The attack should be blocked by gates
        # If this passes, the vulnerability is fixed

    @pytest.mark.parametrize("malicious_input", [
        {{"event": "{attack_event}", "bypass_attempt": True}},
        {{"event": "{attack_event}", "force": True}},
        {{"event": "{attack_event}", "skip_auth": True}},
    ])
    def test_bypass_attempts_blocked(self, state_machine, malicious_input):
        """
        Test that various bypass attempts are blocked.

        These represent different ways an attacker might try to
        circumvent the security controls.
        """
        # Attempt bypass with malicious payload
        result = state_machine.dispatch(
            malicious_input["event"],
            malicious_input
        )

        # Document the attempt (test is informational)
        assert result is not None, "Dispatch should return a result"
'''


def generate_unittests(params: dict) -> dict:
    """
    Generate pytest-compatible unit tests for each vulnerability.

    Creates test cases that:
    1. Demonstrate the vulnerability exists (attack succeeds)
    2. Verify gates block unauthorized access (after fix)
    3. Test various bypass attempts
    """
    counter_examples = params.get("counter_examples", [])
    bone_json = params.get("bone_json", {})
    target_path = params.get("target_path", "")
    target_name = params.get("target_name", "target")
    output_dir = params.get("output_dir", "")

    if not counter_examples:
        return {
            "test_file": None,
            "test_count": 0,
            "error": None
        }

    # Create tests directory
    tests_dir = os.path.join(output_dir, "tests")
    os.makedirs(tests_dir, exist_ok=True)

    # Generate individual test cases
    test_cases = []
    regression_tests = []

    for i, ce in enumerate(counter_examples):
        test_class_name = _sanitize_class_name(ce.get("id", f"Vuln{i}"))
        vulnerability_name = ce.get("invariant_name", "Unknown Vulnerability")
        severity = ce.get("severity", "unknown")
        attack_vector = ce.get("attack_vector", "Unknown")
        trace = ce.get("trace", [])

        # Generate trace comment
        trace_comment = ""
        for step in trace:
            trace_comment += f"        #   State {step.get('state_num', '?')}: {step.get('state', 'unknown')}"
            if step.get("event"):
                trace_comment += f" (via {step['event']})"
            trace_comment += "\n"

        # Generate test steps
        test_steps = _generate_test_steps(ce, bone_json)
        gate_test_steps = _generate_gate_test_steps(ce, bone_json)

        # Get attack event
        attack_event = "ATTACK"
        if trace and len(trace) > 1:
            attack_event = trace[-1].get("event", "ATTACK")

        # Generate invariant expression
        invariant_expr = ce.get("tla_expression", "state != 'vulnerable'")

        test_case = SINGLE_TEST_TEMPLATE.format(
            test_class_name=test_class_name,
            vulnerability_name=vulnerability_name,
            severity=severity,
            attack_vector=attack_vector,
            trace_comment=trace_comment,
            test_steps=test_steps,
            min_transitions=max(1, len(trace) - 1),
            invariant_expression=invariant_expr,
            gate_test_steps=gate_test_steps,
            attack_event=attack_event
        )

        test_cases.append(test_case)

        # Add to regression test class
        regression_tests.append(f'''
    def test_{test_class_name.lower()}_regression(self, state_machine):
        """Regression test for {vulnerability_name}."""
        # This test documents the vulnerability
        # Mark as xfail after fix is applied
        state_machine.dispatch("{attack_event}")
        assert state_machine.state is not None
''')

    # Generate imports
    imports = _generate_test_imports(target_path, bone_json)

    # Assemble full test file
    test_file_content = UNITTEST_TEMPLATE.format(
        target_name=target_name,
        timestamp=datetime.now().isoformat(),
        test_file=f"test_{target_name}_security.py",
        imports=imports,
        test_cases="\n\n".join(test_cases),
        regression_tests="\n".join(regression_tests)
    )

    # Write test file
    test_file_path = os.path.join(tests_dir, f"test_{target_name}_security.py")
    with open(test_file_path, "w") as f:
        f.write(test_file_content)

    # Generate conftest.py for pytest fixtures
    conftest_content = _generate_conftest(target_name, bone_json)
    conftest_path = os.path.join(tests_dir, "conftest.py")
    with open(conftest_path, "w") as f:
        f.write(conftest_content)

    # Generate pytest.ini
    pytest_ini = f'''[pytest]
testpaths = .
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = -v --tb=short
markers =
    security: Security vulnerability tests
    regression: Regression tests for fixed vulnerabilities
    slow: Slow running tests
'''
    pytest_ini_path = os.path.join(tests_dir, "pytest.ini")
    with open(pytest_ini_path, "w") as f:
        f.write(pytest_ini)

    return {
        "test_file": test_file_path,
        "test_count": len(counter_examples),
        "tests_dir": tests_dir,
        "error": None
    }


def _sanitize_class_name(name: str) -> str:
    """Convert a name to a valid Python class name."""
    # Remove non-alphanumeric characters
    sanitized = re.sub(r'[^a-zA-Z0-9]', '', name)
    # Ensure it starts with uppercase
    if sanitized and sanitized[0].isdigit():
        sanitized = "Test" + sanitized
    return sanitized.title().replace("_", "")


def _generate_test_steps(counter_example: dict, bone_json: dict) -> str:
    """Generate pytest test steps from counter-example trace."""
    lines = []
    trace = counter_example.get("trace", [])
    meta = bone_json.get("_lvp_meta", {})
    functions = meta.get("functions", [])

    lines.append("        # Execute attack sequence")

    for i, step in enumerate(trace):
        event = step.get("event", "")
        target_state = step.get("state", "")

        if event:
            lines.append(f"        ")
            lines.append(f"        # Step {i + 1}: {event} -> {target_state}")
            lines.append(f"        state_machine.dispatch(\"{event}\")")
        else:
            lines.append(f"        # Initial state: {target_state}")

    if not trace:
        lines.append("        # No specific attack steps in trace")
        lines.append("        pass")

    return "\n".join(lines)


def _generate_gate_test_steps(counter_example: dict, bone_json: dict) -> str:
    """Generate test steps that verify gates block the attack."""
    lines = []
    trace = counter_example.get("trace", [])

    lines.append("        # Attempt attack without authorization")
    lines.append("        # Gates should block this sequence")

    if trace and len(trace) > 1:
        # Get the attack event
        attack_event = trace[-1].get("event", "ATTACK")
        lines.append(f"        ")
        lines.append(f"        # Try to trigger vulnerable transition")
        lines.append(f"        can_proceed = state_machine.can_transition(\"{attack_event}\")")
        lines.append(f"        ")
        lines.append(f"        # After fix: this should be blocked")
        lines.append(f"        # assert not can_proceed, \"Gate should block unauthorized access\"")
    else:
        lines.append("        pass  # No specific gate test for this trace")

    return "\n".join(lines)


def _generate_test_imports(target_path: str, bone_json: dict) -> str:
    """Generate import statements for test file."""
    lines = []

    if target_path:
        target_name = Path(target_path).stem
        parent_name = Path(target_path).parent.name
        lines.append(f"# Target module imports")
        lines.append(f"# Uncomment after implementing actual state machine")
        lines.append(f"# from {parent_name}.{target_name} import *")

    # Add common test imports
    lines.append("")
    lines.append("# Test utilities")
    lines.append("from contextlib import contextmanager")

    return "\n".join(lines)


def _generate_conftest(target_name: str, bone_json: dict) -> str:
    """Generate pytest conftest.py with shared fixtures."""
    states = list(bone_json.get("states", {}).keys())
    transitions = bone_json.get("transitions", [])

    return f'''"""
LVP Security Test Configuration
Target: {target_name}

Shared fixtures and configuration for security tests.
"""

import pytest
from typing import Dict, List, Any
from dataclasses import dataclass, field


@dataclass
class StateContext:
    """Context for state machine testing."""
    state: str = "idle"
    variables: Dict[str, Any] = field(default_factory=dict)
    history: List[Dict] = field(default_factory=list)

    def set(self, key: str, value: Any):
        self.variables[key] = value

    def get(self, key: str, default: Any = None):
        return self.variables.get(key, default)


class StateMachineHarness:
    """
    Test harness for state machine security testing.

    This harness simulates the state machine behavior and
    tracks transitions for verification.
    """

    STATES = {states!r}

    def __init__(self):
        self.state = "idle"
        self.context = StateContext()
        self.transitions_taken = []
        self.gates_checked = []
        self.blocked_transitions = []

    def dispatch(self, event: str, payload: dict = None) -> bool:
        """Dispatch an event and track the transition."""
        self.transitions_taken.append({{
            "event": event,
            "from_state": self.state,
            "payload": payload
        }})
        return True

    def check_gate(self, gate_name: str, condition: bool) -> bool:
        """Check a gate condition."""
        self.gates_checked.append({{
            "gate": gate_name,
            "result": condition
        }})
        if not condition:
            self.blocked_transitions.append(gate_name)
        return condition

    def assert_blocked(self, gate_name: str):
        """Assert that a specific gate blocked a transition."""
        assert gate_name in self.blocked_transitions, \\
            f"Expected gate '{{gate_name}}' to block transition"

    def assert_reached_state(self, expected_state: str):
        """Assert the state machine reached a specific state."""
        assert self.state == expected_state, \\
            f"Expected state '{{expected_state}}', got '{{self.state}}'"


@pytest.fixture
def harness():
    """Provide a fresh state machine harness for each test."""
    return StateMachineHarness()


@pytest.fixture
def vulnerable_context():
    """Context that simulates vulnerable conditions."""
    ctx = StateContext()
    ctx.set("authorized", False)
    ctx.set("authenticated", False)
    ctx.set("validated", False)
    return ctx


@pytest.fixture
def secure_context():
    """Context that simulates secure conditions."""
    ctx = StateContext()
    ctx.set("authorized", True)
    ctx.set("authenticated", True)
    ctx.set("validated", True)
    return ctx


def pytest_configure(config):
    """Register custom markers."""
    config.addinivalue_line(
        "markers", "security: mark test as security-related"
    )
    config.addinivalue_line(
        "markers", "vulnerability(id): associate test with vulnerability ID"
    )
'''


def _generate_exploit_runner(exploits: List[dict], exploits_dir: str, target_name: str) -> str:
    """Generate a master script to run all exploits."""
    runner_code = f'''#!/usr/bin/env python3
"""
LVP Exploit Runner
Target: {target_name}
Exploits: {len(exploits)}

Run all PoC scripts and generate a summary report.
"""

import os
import sys
import subprocess
from pathlib import Path

def main():
    print("=" * 60)
    print(f"LVP Exploit Runner - {target_name}")
    print("=" * 60)

    exploits_dir = Path(__file__).parent
    results = []

'''

    for exploit in exploits:
        exploit_file = Path(exploit["path"]).name
        runner_code += f'''
    # Run {exploit["id"]}
    print(f"\\n[*] Running {exploit["id"]}...")
    try:
        result = subprocess.run(
            [sys.executable, str(exploits_dir / "{exploit_file}")],
            capture_output=True,
            text=True,
            timeout=30
        )
        success = result.returncode == 0
        results.append(("{exploit["id"]}", "{exploit["severity"]}", success))
        print(result.stdout)
        if result.stderr:
            print(result.stderr, file=sys.stderr)
    except Exception as e:
        results.append(("{exploit["id"]}", "{exploit["severity"]}", False))
        print(f"[!] Error: {{e}}")
'''

    runner_code += '''
    # Summary
    print("\\n" + "=" * 60)
    print("EXPLOIT SUMMARY")
    print("=" * 60)

    vulnerable = sum(1 for _, _, s in results if s)
    for name, severity, success in results:
        status = "VULNERABLE" if success else "NOT VULNERABLE"
        print(f"  [{severity.upper():8}] {name}: {status}")

    print(f"\\nTotal: {vulnerable}/{len(results)} exploits succeeded")

    return vulnerable > 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
'''

    runner_path = os.path.join(exploits_dir, "run_all_exploits.py")
    with open(runner_path, "w") as f:
        f.write(runner_code)

    # Make executable
    os.chmod(runner_path, 0o755)

    return runner_path


# ============================================================================
# PHASE 5: THE FIX - Gate Insertion and TLAPS Verification
# ============================================================================

def generate_patches(params: dict) -> dict:
    """
    Phase 5: The Fix - Generate patches to add missing gates.

    For each vulnerability, generates:
    - A modified blueprint with the missing gate
    - A diff showing the change
    - Suggested code modifications
    """
    counter_examples = params.get("counter_examples", [])
    bone_json = params.get("bone_json", {})
    invariants = params.get("invariants", [])
    api_key = params.get("api_key", "")
    api_base = params.get("api_base", "")
    model = params.get("model", "gpt-4")
    output_dir = params.get("output_dir", "")

    if not counter_examples:
        return {
            "patches": [],
            "patched_json": bone_json,
            "error": None
        }

    patches = []
    patched_json = json.loads(json.dumps(bone_json))  # Deep copy

    for ce in counter_examples:
        # Find the vulnerable transition
        trace = ce.get("trace", [])
        if len(trace) < 2:
            continue

        # Get the transition that led to violation
        last_state = trace[-1].get("state", "")
        prev_state = trace[-2].get("state", "") if len(trace) > 1 else "*"
        event = trace[-1].get("event", "")

        # Find matching transition in blueprint
        vuln_trans = None
        for trans in patched_json.get("transitions", []):
            if (trans.get("to") == last_state and
                (trans.get("from") == prev_state or trans.get("from") == "*")):
                vuln_trans = trans
                break

        if not vuln_trans:
            continue

        # Generate a fix gate
        gate_id = f"fix_gate_{ce.get('id', 'unknown').lower()}"

        # Determine gate expression based on invariant
        inv_id = ce.get("violated_invariant", "")
        inv = next((i for i in invariants if i.get("id") == inv_id), {})
        tla_expr = inv.get("tla_expression", "")

        # Convert TLA+ expression to Python-like gate expression
        gate_expr = _tla_to_python_gate(tla_expr, ce)

        # Add gate to blueprint
        if "gates" not in patched_json:
            patched_json["gates"] = {}

        patched_json["gates"][gate_id] = {
            "type": "expression",
            "expression": gate_expr,
            "description": f"Security fix for {ce.get('invariant_name', 'vulnerability')}"
        }

        # Add gate to transition
        if "gates" not in vuln_trans:
            vuln_trans["gates"] = []
        vuln_trans["gates"].append(gate_id)

        patches.append({
            "id": f"PATCH_{len(patches) + 1:03d}",
            "fixes": ce.get("id"),
            "vulnerability": ce.get("invariant_name"),
            "gate_added": gate_id,
            "gate_expression": gate_expr,
            "transition_modified": vuln_trans.get("id"),
            "description": f"Add authorization gate to transition {vuln_trans.get('id')}"
        })

    # Remove LVP meta before saving
    patched_clean = {k: v for k, v in patched_json.items() if not k.startswith("_")}

    # Save patched blueprint
    if output_dir:
        patched_path = os.path.join(output_dir, "patched_bone.json")
        with open(patched_path, "w") as f:
            json.dump(patched_clean, f, indent=2)

        # Save patches
        patches_path = os.path.join(output_dir, "patches.json")
        with open(patches_path, "w") as f:
            json.dump(patches, f, indent=2)

    return {
        "patches": patches,
        "patched_json": patched_json,
        "error": None
    }


def _tla_to_python_gate(tla_expr: str, counter_example: dict) -> str:
    """Convert TLA+ expression to Python gate expression."""
    if not tla_expr:
        # Generate a sensible default based on the vulnerability
        attack_vector = counter_example.get("attack_vector", "").lower()

        if "authorization" in attack_vector or "approval" in attack_vector:
            return "authorized == True"
        elif "authentication" in attack_vector:
            return "authenticated == True"
        elif "validation" in attack_vector:
            return "validated == True"
        else:
            return "security_check_passed == True"

    # Convert TLA+ to Python
    py_expr = tla_expr

    # TLA+ to Python translations
    translations = [
        ("\\\\in", "in"),
        (" /\\\\ ", " and "),
        (" \\\\/ ", " or "),
        ("~", "not "),
        (" /= ", " != "),
        (" = ", " == "),
        ("NULL", "None"),
        ("TRUE", "True"),
        ("FALSE", "False"),
        ("=>", "or not"),  # Implication: A => B is equivalent to (not A) or B
    ]

    for tla_op, py_op in translations:
        py_expr = py_expr.replace(tla_op, py_op)

    return py_expr


def verify_fix_tlaps(params: dict) -> dict:
    """
    Verify the fix using TLAPS (TLA+ Proof System).

    Generates a proof that the patched blueprint satisfies all invariants.
    """
    patched_json = params.get("patched_json", {})
    invariants = params.get("invariants", [])
    output_dir = params.get("output_dir", "")
    lpp_root = params.get("lpp_root", "")

    if not patched_json:
        return {
            "tlaps_proof": None,
            "fix_verified": False,
            "error": "No patched blueprint to verify"
        }

    # Re-run TLC on the patched blueprint
    try:
        tla_result = generate_tla({
            "bone_json": patched_json,
            "invariants": invariants,
            "output_dir": output_dir,
            "lpp_root": lpp_root
        })

        if tla_result.get("error"):
            return {
                "tlaps_proof": None,
                "fix_verified": False,
                "error": tla_result["error"]
            }

        tlc_result = run_tlc({
            "tla_path": tla_result.get("tla_path"),
            "tla_spec": tla_result.get("tla_spec"),
            "invariants": invariants,
            "output_dir": output_dir,
            "lpp_root": lpp_root,
            "bone_json": patched_json
        })

        # Check if any vulnerabilities remain
        remaining_vulns = tlc_result.get("vulnerability_count", 0)

        # Generate proof document
        proof = {
            "status": "verified" if remaining_vulns == 0 else "partial",
            "original_vulnerabilities": len(invariants),
            "remaining_vulnerabilities": remaining_vulns,
            "patches_applied": True,
            "tlc_passed": remaining_vulns == 0,
            "timestamp": datetime.now().isoformat()
        }

        # Save proof
        if output_dir:
            proof_path = os.path.join(output_dir, "tlaps_proof.json")
            with open(proof_path, "w") as f:
                json.dump(proof, f, indent=2)

        return {
            "tlaps_proof": json.dumps(proof, indent=2),
            "fix_verified": remaining_vulns == 0,
            "error": None
        }

    except Exception as e:
        return {
            "tlaps_proof": None,
            "fix_verified": False,
            "error": f"TLAPS verification failed: {str(e)}"
        }


# ============================================================================
# REPORTING
# ============================================================================

def generate_report(params: dict) -> dict:
    """Generate the final security audit report."""
    target_name = params.get("target_name", "unknown")
    target_path = params.get("target_path", "")
    bone_json = params.get("bone_json", {})
    threat_model = params.get("threat_model", {})
    invariants = params.get("invariants", [])
    counter_examples = params.get("counter_examples", [])
    vulnerability_count = params.get("vulnerability_count", 0)
    exploits = params.get("exploits", [])
    patches = params.get("patches", [])
    fix_verified = params.get("fix_verified", False)
    severity_score = params.get("severity_score", 0)
    output_dir = params.get("output_dir", "")
    run_id = params.get("run_id", "")

    report = {
        "lvp_report": {
            "version": "1.0.0",
            "run_id": run_id,
            "timestamp": datetime.now().isoformat(),
            "target": {
                "name": target_name,
                "path": target_path,
                "states": len(bone_json.get("states", {})),
                "transitions": len(bone_json.get("transitions", [])),
                "source_lines": bone_json.get("_lvp_meta", {}).get("source_lines", 0)
            }
        },
        "summary": {
            "severity_score": severity_score,
            "vulnerabilities_found": vulnerability_count,
            "invariants_checked": len(invariants),
            "exploits_generated": len(exploits),
            "patches_generated": len(patches),
            "fix_verified": fix_verified,
            "verdict": _calculate_verdict(severity_score, vulnerability_count, fix_verified)
        },
        "threat_model": threat_model,
        "invariants": invariants,
        "vulnerabilities": [
            {
                "id": ce.get("id"),
                "name": ce.get("invariant_name"),
                "severity": ce.get("severity"),
                "attack_vector": ce.get("attack_vector"),
                "exploitable": ce.get("exploitability", "unknown"),
                "trace_length": len(ce.get("trace", []))
            }
            for ce in counter_examples
        ],
        "exploits": [
            {
                "id": exp.get("id"),
                "vulnerability": exp.get("vulnerability"),
                "severity": exp.get("severity"),
                "poc_path": exp.get("path")
            }
            for exp in exploits
        ],
        "patches": patches,
        "recommendations": _generate_recommendations(counter_examples, patches, fix_verified)
    }

    # Save report
    if output_dir:
        report_path = os.path.join(output_dir, "lvp_report.json")
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)

        # Generate markdown report
        md_report = _generate_markdown_report(report)
        md_path = os.path.join(output_dir, "lvp_report.md")
        with open(md_path, "w") as f:
            f.write(md_report)

    return {"audit_report": report, "error": None}


def _calculate_verdict(severity_score: float, vuln_count: int, fix_verified: bool) -> str:
    """Calculate overall security verdict."""
    if vuln_count == 0:
        return "SECURE"
    elif fix_verified:
        return "FIXED"
    elif severity_score >= 8:
        return "CRITICAL"
    elif severity_score >= 5:
        return "WARNING"
    else:
        return "REVIEW"


def _generate_recommendations(counter_examples: List[dict], patches: List[dict], fix_verified: bool) -> List[str]:
    """Generate security recommendations."""
    recommendations = []

    if not counter_examples:
        recommendations.append("No vulnerabilities found. Continue monitoring for regressions.")
        return recommendations

    # Group by severity
    critical = [ce for ce in counter_examples if ce.get("severity") == "critical"]
    high = [ce for ce in counter_examples if ce.get("severity") == "high"]

    if critical:
        recommendations.append(
            f"CRITICAL: Address {len(critical)} critical vulnerabilities immediately. "
            "These represent serious security risks."
        )

    if high:
        recommendations.append(
            f"HIGH: Review {len(high)} high-severity issues. "
            "These should be fixed in the next release."
        )

    if patches and not fix_verified:
        recommendations.append(
            "Apply the generated patches and re-run verification. "
            "Manual review recommended before deployment."
        )

    if fix_verified:
        recommendations.append(
            "All patches verified. Run full regression tests before deployment."
        )

    recommendations.append(
        "Consider adding formal verification to CI/CD pipeline to prevent regressions."
    )

    return recommendations


def _generate_markdown_report(report: dict) -> str:
    """Generate a human-readable markdown report."""
    summary = report.get("summary", {})
    target = report.get("lvp_report", {}).get("target", {})
    vulns = report.get("vulnerabilities", [])

    md = f"""# LVP Security Audit Report

## Target: {target.get('name', 'Unknown')}

**Generated:** {report.get('lvp_report', {}).get('timestamp', 'Unknown')}
**Run ID:** {report.get('lvp_report', {}).get('run_id', 'Unknown')}

---

## Executive Summary

| Metric | Value |
|--------|-------|
| Severity Score | **{summary.get('severity_score', 0)}/10** |
| Vulnerabilities Found | {summary.get('vulnerabilities_found', 0)} |
| Invariants Checked | {summary.get('invariants_checked', 0)} |
| Exploits Generated | {summary.get('exploits_generated', 0)} |
| Patches Generated | {summary.get('patches_generated', 0)} |
| Fix Verified | {'Yes' if summary.get('fix_verified') else 'No'} |
| **Verdict** | **{summary.get('verdict', 'UNKNOWN')}** |

---

## Vulnerabilities

"""

    if vulns:
        for vuln in vulns:
            md += f"""### {vuln.get('id', 'Unknown')} - {vuln.get('name', 'Unknown')}

- **Severity:** {vuln.get('severity', 'Unknown').upper()}
- **Attack Vector:** {vuln.get('attack_vector', 'Unknown')}
- **Exploitable:** {vuln.get('exploitable', 'Unknown')}
- **Trace Length:** {vuln.get('trace_length', 0)} states

"""
    else:
        md += "*No vulnerabilities found.*\n\n"

    md += """---

## Recommendations

"""

    for i, rec in enumerate(report.get("recommendations", []), 1):
        md += f"{i}. {rec}\n"

    md += """
---

*Generated by L++ Logic Vulnerability Pointer (LVP)*
"""

    return md


def generate_secure_report(params: dict) -> dict:
    """Generate report for targets that passed all checks."""
    target_name = params.get("target_name", "unknown")
    target_path = params.get("target_path", "")
    bone_json = params.get("bone_json", {})
    invariants = params.get("invariants", [])
    output_dir = params.get("output_dir", "")
    run_id = params.get("run_id", "")

    report = {
        "lvp_report": {
            "version": "1.0.0",
            "run_id": run_id,
            "timestamp": datetime.now().isoformat(),
            "target": {
                "name": target_name,
                "path": target_path
            }
        },
        "summary": {
            "severity_score": 0,
            "vulnerabilities_found": 0,
            "invariants_checked": len(invariants),
            "verdict": "SECURE"
        },
        "message": "Target passed all security checks. No logic vulnerabilities detected."
    }

    if output_dir:
        report_path = os.path.join(output_dir, "lvp_report.json")
        with open(report_path, "w") as f:
            json.dump(report, f, indent=2)

    return {"audit_report": report}


def capture_error(params: dict) -> dict:
    """Capture and log errors."""
    error = params.get("error", "Unknown error")
    phase = params.get("phase", "unknown")
    output_dir = params.get("output_dir", "")

    report = {
        "lvp_report": {
            "status": "error",
            "timestamp": datetime.now().isoformat()
        },
        "error": {
            "phase": phase,
            "message": error
        }
    }

    if output_dir:
        error_path = os.path.join(output_dir, "error_report.json")
        with open(error_path, "w") as f:
            json.dump(report, f, indent=2)

    return {"audit_report": report}


# ============================================================================
# LLM-POWERED TEST GENERATION - Direct Code Testing
# ============================================================================

LLM_TEST_PROMPT = '''You are a security test engineer. Generate pytest test code that directly calls the target module's functions to demonstrate a vulnerability.

## Target Module
- **Module**: {target_name}
- **Path**: {target_path}

## Vulnerability
- **Type**: {vuln_type}
- **Severity**: {severity}
- **Description**: {description}
- **Affected Function**: {affected_function}

## Logic Graph (State Machine)
States: {states}
Transitions: {transitions}

## Function Signatures from Source
{function_signatures}

## Source Code Excerpt
```python
{source_excerpt}
```

## Task
Generate a complete pytest test class that:
1. Imports the actual target module
2. Creates test fixtures with realistic data
3. Calls the actual vulnerable function(s)
4. Demonstrates the vulnerability with concrete inputs
5. Includes assertions that verify the flaw exists

Output ONLY the Python test code, no explanation. The test should be runnable as-is.
'''

LLM_TEST_TEMPLATE = '''#!/usr/bin/env python3
"""
LVP Direct Security Test: {target_name}
Generated: {timestamp}
Mode: LLM-Powered (tests actual code, not mock)

These tests call the actual target module functions to verify vulnerabilities.
Run with: pytest {test_file} -v
"""

import pytest
import sys
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock
from typing import Any, Dict, List

# Add target to path
TARGET_PATH = "{target_path}"
sys.path.insert(0, str(Path(TARGET_PATH).parent.parent))

# Import target module
try:
    from {import_path} import *
    TARGET_AVAILABLE = True
except ImportError as e:
    TARGET_AVAILABLE = False
    IMPORT_ERROR = str(e)


@pytest.fixture
def skip_if_unavailable():
    """Skip tests if target module cannot be imported."""
    if not TARGET_AVAILABLE:
        pytest.skip(f"Target module unavailable: {{IMPORT_ERROR}}")


# ============================================================================
# LLM-Generated Test Cases
# ============================================================================

{llm_generated_tests}


# ============================================================================
# Fallback Static Tests (if LLM unavailable)
# ============================================================================

{static_tests}


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
'''


def generate_llm_unittests(params: dict) -> dict:
    """
    Generate pytest tests using LLM to create tests that call actual target code.

    This is the "universal" test generation that:
    1. Analyzes the logic graph (bone_json)
    2. Reads the actual source code
    3. Uses LLM to generate tests with concrete inputs
    4. Creates tests that import and call the real functions
    """
    counter_examples = params.get("counter_examples", [])
    bone_json = params.get("bone_json", {})
    target_path = params.get("target_path", "")
    target_name = params.get("target_name", "target")
    output_dir = params.get("output_dir", "")
    api_key = params.get("api_key", os.environ.get("OPENAI_API_KEY", ""))
    api_base = params.get("api_base", os.environ.get("OPENAI_API_BASE", "https://api.openai.com/v1"))
    model = params.get("model", os.environ.get("LLM_MODEL", "gpt-4"))

    if not counter_examples:
        return {"test_file": None, "test_count": 0, "error": None}

    # Create tests directory
    tests_dir = os.path.join(output_dir, "tests")
    os.makedirs(tests_dir, exist_ok=True)

    # Read source code
    source_code = ""
    if target_path and os.path.exists(target_path):
        with open(target_path, "r") as f:
            source_code = f.read()

    # Extract function signatures from bone_json
    meta = bone_json.get("_lvp_meta", {})
    functions = meta.get("functions", [])
    function_sigs = _extract_function_signatures(source_code, functions)

    # Map states to functions
    state_to_function = _map_states_to_functions(bone_json)

    # Generate tests for each vulnerability
    llm_tests = []
    static_tests = []

    for i, ce in enumerate(counter_examples):
        vuln_type = ce.get("violated_invariant", "UNKNOWN")
        severity = ce.get("severity", "medium")
        description = ce.get("invariant_name", "Unknown vulnerability")

        # Find affected function from state mapping
        trace = ce.get("trace", [])
        affected_state = trace[-1].get("state", "") if trace else ""
        affected_function = state_to_function.get(affected_state, affected_state)

        # Try LLM generation if API key available
        if api_key:
            llm_test = _generate_single_llm_test(
                target_name=target_name,
                target_path=target_path,
                vuln_type=vuln_type,
                severity=severity,
                description=description,
                affected_function=affected_function,
                bone_json=bone_json,
                function_sigs=function_sigs,
                source_code=source_code[:3000],  # First 3k chars
                api_key=api_key,
                api_base=api_base,
                model=model
            )
            if llm_test:
                llm_tests.append(llm_test)
                continue

        # Fallback: Generate static test based on function analysis
        static_test = _generate_static_direct_test(
            ce_id=ce.get("id", f"CE_{i:03d}"),
            vuln_type=vuln_type,
            severity=severity,
            description=description,
            affected_function=affected_function,
            function_sigs=function_sigs,
            bone_json=bone_json
        )
        static_tests.append(static_test)

    # Build import path
    import_path = _get_import_path(target_path)

    # Assemble full test file
    test_content = LLM_TEST_TEMPLATE.format(
        target_name=target_name,
        timestamp=datetime.now().isoformat(),
        test_file=f"test_{target_name}_direct.py",
        target_path=target_path,
        import_path=import_path,
        llm_generated_tests="\n\n".join(llm_tests) if llm_tests else "# No LLM tests generated (API key not provided)",
        static_tests="\n\n".join(static_tests) if static_tests else "# No static tests generated"
    )

    # Write test file
    test_file_path = os.path.join(tests_dir, f"test_{target_name}_direct.py")
    with open(test_file_path, "w") as f:
        f.write(test_content)

    return {
        "test_file": test_file_path,
        "test_count": len(llm_tests) + len(static_tests),
        "llm_tests": len(llm_tests),
        "static_tests": len(static_tests),
        "tests_dir": tests_dir,
        "error": None
    }


def _extract_function_signatures(source_code: str, functions: list) -> dict:
    """Extract function signatures from source code."""
    sigs = {}
    for func in functions:
        # Handle both string and dict formats
        if isinstance(func, str):
            func_name = func
        elif isinstance(func, dict):
            func_name = func.get("name", "")
        else:
            continue

        if not func_name:
            continue

        # Find function definition in source
        pattern = rf'def\s+{re.escape(func_name)}\s*\([^)]*\)'
        match = re.search(pattern, source_code)
        if match:
            sigs[func_name] = match.group(0)
        else:
            sigs[func_name] = f"def {func_name}(params: dict)"

    return sigs


def _map_states_to_functions(bone_json: dict) -> dict:
    """Map state names to their corresponding functions."""
    mapping = {}
    states = bone_json.get("states", {})

    for state_name, state_info in states.items():
        desc = state_info.get("description", "")
        # Parse "From function: xyz" pattern
        if "From function:" in desc:
            func_name = desc.split("From function:")[-1].strip()
            mapping[state_name] = func_name
        else:
            # Try to derive from state name
            mapping[state_name] = state_name.replace("-", "_")

    return mapping


def _get_import_path(target_path: str) -> str:
    """Generate Python import path from file path."""
    if not target_path:
        return "target"

    path = Path(target_path)
    # Get parent.name (e.g., "src") and file stem
    parent_name = path.parent.name
    module_name = path.stem

    return f"{parent_name}.{module_name}"


def _generate_single_llm_test(
    target_name: str,
    target_path: str,
    vuln_type: str,
    severity: str,
    description: str,
    affected_function: str,
    bone_json: dict,
    function_sigs: dict,
    source_code: str,
    api_key: str,
    api_base: str,
    model: str
) -> str:
    """Generate a single test using LLM."""
    try:
        import requests

        prompt = LLM_TEST_PROMPT.format(
            target_name=target_name,
            target_path=target_path,
            vuln_type=vuln_type,
            severity=severity,
            description=description,
            affected_function=affected_function,
            states=list(bone_json.get("states", {}).keys()),
            transitions=[t.get("id") for t in bone_json.get("transitions", [])],
            function_signatures=json.dumps(function_sigs, indent=2),
            source_excerpt=source_code
        )

        response = requests.post(
            f"{api_base}/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": model,
                "messages": [
                    {"role": "system", "content": "You are a security test engineer. Output only Python code."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.3,
                "max_tokens": 2000
            },
            timeout=30
        )

        if response.status_code == 200:
            result = response.json()
            code = result["choices"][0]["message"]["content"]
            # Clean up markdown code blocks if present
            if "```python" in code:
                code = code.split("```python")[1].split("```")[0]
            elif "```" in code:
                code = code.split("```")[1].split("```")[0]
            return code.strip()

    except Exception as e:
        pass  # Fall through to static generation

    return ""


def _generate_static_direct_test(
    ce_id: str,
    vuln_type: str,
    severity: str,
    description: str,
    affected_function: str,
    function_sigs: dict,
    bone_json: dict
) -> str:
    """Generate a static test that calls the actual function."""
    class_name = _sanitize_class_name(ce_id)
    func_sig = function_sigs.get(affected_function, f"def {affected_function}(params: dict)")

    # Extract parameters from signature
    param_match = re.search(r'\((.*?)\)', func_sig)
    params = param_match.group(1) if param_match else "params: dict"

    return f'''
class TestDirect{class_name}:
    """
    Direct test for: {description}
    Severity: {severity}
    Function: {affected_function}
    """

    @pytest.fixture
    def test_params(self):
        """Create test parameters for {affected_function}."""
        return {{
            "test_mode": True,
            "bypass_checks": False,  # Set True to test vulnerability
            "_test_vulnerability": "{vuln_type}"
        }}

    def test_{affected_function}_vulnerability(self, skip_if_unavailable, test_params):
        """
        Test that {affected_function} has vulnerability: {vuln_type}

        This test calls the actual function to verify the flaw.
        If the vulnerability is fixed, this test should FAIL.
        """
        if not TARGET_AVAILABLE:
            pytest.skip("Target not available")

        # Call actual function
        try:
            result = {affected_function}(test_params)

            # Vulnerability check: function executed without proper validation
            # After fix, this should raise an exception or return error
            assert result is not None, "Function should return a result"

        except PermissionError:
            pytest.fail("Good! Gate blocked unauthorized access (vulnerability fixed)")
        except ValueError as e:
            if "unauthorized" in str(e).lower() or "permission" in str(e).lower():
                pytest.fail("Good! Validation blocked attack (vulnerability fixed)")
            raise

    def test_{affected_function}_with_bypass_attempt(self, skip_if_unavailable, test_params):
        """Test bypass attempt is blocked after fix."""
        if not TARGET_AVAILABLE:
            pytest.skip("Target not available")

        # Attempt to bypass security
        test_params["force"] = True
        test_params["skip_auth"] = True

        try:
            result = {affected_function}(test_params)
            # If we get here without error, vulnerability may exist
            assert result is not None
        except Exception:
            pass  # Expected after fix
'''


# ============================================================================
# COMPUTE REGISTRY
# ============================================================================

COMPUTE_REGISTRY = {
    # Phase 0: Init
    "lvp:init": init,
    "lvp:set_target_name": set_target_name,

    # Phase 1: X-Ray
    "lvp:extract_logic": extract_logic,

    # Phase 2: Threat Model
    "lvp:define_invariants": define_invariants,

    # Phase 3: Stress Test
    "lvp:generate_tla": generate_tla,
    "lvp:run_tlc": run_tlc,
    "lvp:analyze_traces": analyze_traces,

    # Phase 4: Exploit Gen
    "lvp:generate_exploits": generate_exploits,
    "lvp:generate_unittests": generate_unittests,
    "lvp:generate_llm_unittests": generate_llm_unittests,  # LLM-powered direct tests

    # Phase 5: The Fix
    "lvp:generate_patches": generate_patches,
    "lvp:verify_fix_tlaps": verify_fix_tlaps,

    # Reporting
    "lvp:generate_report": generate_report,
    "lvp:generate_secure_report": generate_secure_report,
    "lvp:capture_error": capture_error,
}
